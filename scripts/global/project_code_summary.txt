----------------------------------------------
1.  charts/services/demo-api/Chart.yaml
----------------------------------------------

apiVersion: v2
name: demo-api
description: Reusable Helm chart for demo-api with HPA, probes, and rollout support
type: application
version: 0.1.0
appVersion: "1.0.0"


----------------------------------------------
2.  charts/services/demo-api/values.yaml
----------------------------------------------

replicaCount: 2

image:
  repository: ghcr.io/girmaat/demo-api
  tag: "latest"
  pullPolicy: IfNotPresent

service:
  type: ClusterIP
  port: 8080

resources:
  limits:
    cpu: "500m"
    memory: "256Mi"
  requests:
    cpu: "250m"
    memory: "128Mi"

readinessProbe:
  httpGet:
    path: /healthz
    port: http
  initialDelaySeconds: 5
  periodSeconds: 10

livenessProbe:
  httpGet:
    path: /healthz
    port: http
  initialDelaySeconds: 15
  periodSeconds: 20

hpa:
  enabled: true
  minReplicas: 2
  maxReplicas: 5
  targetCPUUtilizationPercentage: 70


----------------------------------------------
3.  charts/services/demo-api/templates/_helpers.tpl
----------------------------------------------

{{- define "demo-api.name" -}}
{{ .Chart.Name }}
{{- end }}

{{- define "demo-api.fullname" -}}
{{ .Release.Name }}-{{ .Chart.Name }}
{{- end }}

{{- define "demo-api.labels" -}}
app.kubernetes.io/name: {{ include "demo-api.name" . }}
app.kubernetes.io/instance: {{ .Release.Name }}
app.kubernetes.io/version: {{ .Chart.AppVersion }}
app.kubernetes.io/managed-by: {{ .Release.Service }}
{{- end }}


----------------------------------------------
4.  charts/services/demo-api/templates/deployment.yaml
----------------------------------------------

apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "demo-api.fullname" . }}
  labels:
{{- include "demo-api.labels" . | nindent 4 }}
spec:
  replicas: {{ .Values.replicaCount }}
  selector:
    matchLabels:
      app: {{ include "demo-api.name" . }}
  template:
    metadata:
      labels:
        app: {{ include "demo-api.name" . }}
    spec:
      containers:
        - name: {{ .Chart.Name }}
          image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          ports:
            - name: http
              containerPort: {{ .Values.service.port }}
              protocol: TCP
{{- if .Values.resources }}
          resources:
{{ toYaml .Values.resources | indent 12 }}
{{- end }}
{{- if .Values.readinessProbe }}
          readinessProbe:
{{ toYaml .Values.readinessProbe | indent 12 }}
{{- end }}
{{- if .Values.livenessProbe }}
          livenessProbe:
{{ toYaml .Values.livenessProbe | indent 12 }}
{{- end }}


----------------------------------------------
5.  charts/services/demo-api/templates/service.yaml
----------------------------------------------

apiVersion: v1
kind: Service
metadata:
  name: {{ include "demo-api.fullname" . }}
  labels: {{- include "demo-api.labels" . | nindent 4 }}
spec:
  type: {{ .Values.service.type }}
  selector:
    app: {{ include "demo-api.name" . }}
  ports:
    - port: 80
      targetPort: {{ .Values.service.port }}
      protocol: TCP


----------------------------------------------
6.  charts/services/demo-api/templates/hpa.yaml
----------------------------------------------

{{- if .Values.hpa.enabled }}
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: {{ include "demo-api.fullname" . }}
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: {{ include "demo-api.fullname" . }}
  minReplicas: {{ .Values.hpa.minReplicas }}
  maxReplicas: {{ .Values.hpa.maxReplicas }}
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: {{ .Values.hpa.targetCPUUtilizationPercentage }}
{{- end }}


----------------------------------------------
7.  overlays/dev/values/demo-api-values.yaml
----------------------------------------------

replicaCount: 1

image:
  tag: dev-latest

resources:
  limits:
    cpu: "250m"
    memory: "128Mi"
  requests:
    cpu: "100m"
    memory: "64Mi"

readinessProbe:
  httpGet:
    path: /healthz
    port: http
  initialDelaySeconds: 10
  periodSeconds: 10

livenessProbe:
  httpGet:
    path: /healthz
    port: http
  initialDelaySeconds: 20
  periodSeconds: 20

hpa:
  enabled: false


----------------------------------------------
8.  overlays/dev/kustomization.yaml
----------------------------------------------

apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

resources:
  - rendered.yaml

commonLabels:
  env: dev


----------------------------------------------
9.  overlays/dev/rendered.yaml
----------------------------------------------

---
# Source: demo-api/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: demo-api-demo-api
  labels:
    app.kubernetes.io/name: demo-api
    app.kubernetes.io/instance: demo-api
    app.kubernetes.io/version: 1.0.0
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  selector:
    app: demo-api
  ports:
    - port: 80
      targetPort: 8080
      protocol: TCP
---
# Source: demo-api/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: demo-api-demo-api
  labels:
    app.kubernetes.io/name: demo-api
    app.kubernetes.io/instance: demo-api
    app.kubernetes.io/version: 1.0.0
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app: demo-api
  template:
    metadata:
      labels:
        app: demo-api
    spec:
      containers:
        - name: demo-api
          image: "ghcr.io/girmaat/demo-api:dev-latest"
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
          resources:
            limits:
              cpu: 250m
              memory: 128Mi
            requests:
              cpu: 100m
              memory: 64Mi
          readinessProbe:
            httpGet:
              path: /healthz
              port: http
            initialDelaySeconds: 10
            periodSeconds: 10
          livenessProbe:
            httpGet:
              path: /healthz
              port: http
            initialDelaySeconds: 20
            periodSeconds: 20


----------------------------------------------
10.  overlays/staging/values/demo-api-values.yaml
----------------------------------------------

replicaCount: 2

image:
  tag: staging-v1.2.3

resources:
  limits:
    cpu: "400m"
    memory: "256Mi"
  requests:
    cpu: "200m"
    memory: "128Mi"

readinessProbe:
  httpGet:
    path: /healthz
    port: http
  initialDelaySeconds: 5
  periodSeconds: 10

livenessProbe:
  httpGet:
    path: /healthz
    port: http
  initialDelaySeconds: 10
  periodSeconds: 20

hpa:
  enabled: true
  minReplicas: 2
  maxReplicas: 4
  targetCPUUtilizationPercentage: 75


----------------------------------------------
11.  overlays/staging/kustomization.yaml
----------------------------------------------

apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

resources:
  - rendered.yaml

commonLabels:
  env: staging


----------------------------------------------
12.  overlays/staging/rendered.yaml
----------------------------------------------

---
# Source: demo-api/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: demo-api-demo-api
  labels:
    app.kubernetes.io/name: demo-api
    app.kubernetes.io/instance: demo-api
    app.kubernetes.io/version: 1.0.0
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  selector:
    app: demo-api
  ports:
    - port: 80
      targetPort: 8080
      protocol: TCP
---
# Source: demo-api/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: demo-api-demo-api
  labels:
    app.kubernetes.io/name: demo-api
    app.kubernetes.io/instance: demo-api
    app.kubernetes.io/version: 1.0.0
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 2
  selector:
    matchLabels:
      app: demo-api
  template:
    metadata:
      labels:
        app: demo-api
    spec:
      containers:
        - name: demo-api
          image: "ghcr.io/girmaat/demo-api:staging-v1.2.0"
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
          resources:
            limits:
              cpu: 400m
              memory: 256Mi
            requests:
              cpu: 200m
              memory: 128Mi
          readinessProbe:
            httpGet:
              path: /healthz
              port: http
            initialDelaySeconds: 5
            periodSeconds: 10
          livenessProbe:
            httpGet:
              path: /healthz
              port: http
            initialDelaySeconds: 10
            periodSeconds: 20
---
# Source: demo-api/templates/hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: demo-api-demo-api
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: demo-api-demo-api
  minReplicas: 2
  maxReplicas: 4
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 75


----------------------------------------------
13.  overlays/prod/values/demo-api-values.yaml
----------------------------------------------

replicaCount: 3

image:
  tag: prod-v1.2.0

resources:
  limits:
    cpu: "500m"
    memory: "512Mi"
  requests:
    cpu: "300m"
    memory: "256Mi"

readinessProbe:
  httpGet:
    path: /healthz
    port: http
  initialDelaySeconds: 3
  periodSeconds: 5

livenessProbe:
  httpGet:
    path: /healthz
    port: http
  initialDelaySeconds: 6
  periodSeconds: 10

hpa:
  enabled: true
  minReplicas: 3
  maxReplicas: 6
  targetCPUUtilizationPercentage: 65


----------------------------------------------
14.  overlays/prod/kustomization.yaml
----------------------------------------------

apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

resources:
  - rendered.yaml

commonLabels:
  env: prod


----------------------------------------------
15.  overlays/prod/rendered.yaml
----------------------------------------------

---
# Source: demo-api/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: demo-api-demo-api
  labels:
    app.kubernetes.io/name: demo-api
    app.kubernetes.io/instance: demo-api
    app.kubernetes.io/version: 1.0.0
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  selector:
    app: demo-api
  ports:
    - port: 80
      targetPort: 8080
      protocol: TCP
---
# Source: demo-api/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: demo-api-demo-api
  labels:
    app.kubernetes.io/name: demo-api
    app.kubernetes.io/instance: demo-api
    app.kubernetes.io/version: 1.0.0
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 3
  selector:
    matchLabels:
      app: demo-api
  template:
    metadata:
      labels:
        app: demo-api
    spec:
      containers:
        - name: demo-api
          image: "ghcr.io/girmaat/demo-api:prod-v1.2.0"
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 300m
              memory: 256Mi
          readinessProbe:
            httpGet:
              path: /healthz
              port: http
            initialDelaySeconds: 3
            periodSeconds: 5
          livenessProbe:
            httpGet:
              path: /healthz
              port: http
            initialDelaySeconds: 6
            periodSeconds: 10
---
# Source: demo-api/templates/hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: demo-api-demo-api
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: demo-api-demo-api
  minReplicas: 3
  maxReplicas: 6
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 65


----------------------------------------------
16.  apps/dev-demo-api.yaml
----------------------------------------------

apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: demo-api-dev
  namespace: argocd
spec:
  project: default

  source:
    repoURL: https://github.com/girmaat/k8s-gitops-progressive-deployment-stack.git
    targetRevision: HEAD
    path: overlays/dev

  destination:
    server: https://kubernetes.default.svc
    namespace: dev

  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
      - CreateNamespace=true


----------------------------------------------
17.  apps/staging-demo-api.yaml
----------------------------------------------

apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: demo-api-staging
  namespace: argocd
spec:
  project: default

  source:
    repoURL: https://github.com/girmaat/k8s-gitops-progressive-deployment-stack.git
    targetRevision: HEAD
    path: overlays/staging

  destination:
    server: https://kubernetes.default.svc
    namespace: staging

  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
      - CreateNamespace=true


----------------------------------------------
18.  apps/prod-demo-api.yaml
----------------------------------------------

apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: demo-api-prod
  namespace: argocd
spec:
  project: default

  source:
    repoURL: https://github.com/girmaat/k8s-gitops-progressive-deployment-stack.git
    targetRevision: HEAD
    path: overlays/prod

  destination:
    server: https://kubernetes.default.svc
    namespace: prod

  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
      - CreateNamespace=true


----------------------------------------------
19.  rollout-strategies/demo-api-rollout.yaml
----------------------------------------------

apiVersion: argoproj.io/v1alpha1
kind: Rollout
metadata:
  name: demo-api
spec:
  replicas: 3
  revisionHistoryLimit: 2
  selector:
    matchLabels:
      app: demo-api
  template:
    metadata:
      labels:
        app: demo-api
    spec:
      containers:
        - name: demo-api
          image: ghcr.io/org/demo-api:{{ .Values.image.tag }}
          ports:
            - containerPort: 8080
  strategy:
    canary:
      maxUnavailable: 1
      maxSurge: 1
      steps:
        - setWeight: 10
        - pause: { duration: 30s }
        - setWeight: 50
        - pause: { duration: 60s }
        - setWeight: 100


----------------------------------------------
20.  policies/restrict-env-label.yaml
----------------------------------------------

[EMPTY FILE]


----------------------------------------------
21.  argo-config/argocd-project.yaml
----------------------------------------------

[EMPTY FILE]


----------------------------------------------
22.  argo-config/image-updater.yaml
----------------------------------------------

[EMPTY FILE]


----------------------------------------------
23.  manifests/ingress-controller.yaml
----------------------------------------------

[EMPTY FILE]


----------------------------------------------
24.  scripts/verify-drift.sh
----------------------------------------------

[EMPTY FILE]


----------------------------------------------
25.  scripts/global/fix-aws-time-skew.sh
----------------------------------------------

#!/bin/bash
set -euo pipefail

BLUE='\033[1;34m'
GREEN='\033[0;32m'
RED='\033[0;31m'
NC='\033[0m'

echo -e "${BLUE}üîß Restarting chronyd...${NC}"
sudo systemctl restart chronyd

echo -e "${BLUE}‚è±Ô∏è Forcing immediate time sync with 'chronyc makestep'...${NC}"
sudo chronyc makestep

echo -e "${BLUE}Checking new system time drift...${NC}"
chronyc tracking

echo -e "${BLUE}Verifying AWS credentials (sts get-caller-identity)...${NC}"
if aws sts get-caller-identity > /dev/null 2>&1; then
  echo -e "${GREEN}[‚úî] AWS credentials are now valid. Time is synced.${NC}"
else
  echo -e "${RED}[‚úò] AWS authentication still failing. Check your credentials or session.${NC}"
  exit 1
fi

----------------------------------------------
26.  scripts/lint-values.sh
----------------------------------------------

[EMPTY FILE]


----------------------------------------------
27.  scripts/promote-image-to-prod.sh
----------------------------------------------

#!/bin/bash

# Define file paths
STAGING_FILE="overlays/staging/values/demo-api-values.yaml"
PROD_FILE="overlays/prod/values/demo-api-values.yaml"

# Extract the current tag from staging values file using yq
TAG=$(yq '.image.tag' "$STAGING_FILE")

# Print what we're about to do
echo "Promoting image tag '$TAG' from staging to prod..."

# Update the prod values file to use the same tag
yq e ".image.tag = \"$TAG\"" -i "$PROD_FILE"

# Stage and commit the change
git add "$PROD_FILE"
git commit -m "Promote demo-api to production: $TAG"


----------------------------------------------
28.  monitoring/dashboards/argo-rollout-status.json
----------------------------------------------

[EMPTY FILE]


----------------------------------------------
29.  monitoring/alerts/drift-detection.yaml
----------------------------------------------

[EMPTY FILE]


----------------------------------------------
30.  monitoring/logging/cloudtrail-ingestor.yaml
----------------------------------------------

[EMPTY FILE]


----------------------------------------------
31.  .gitignore
----------------------------------------------

# OS/system files
.DS_Store
Thumbs.db

# Editor folders
.vscode/
.idea/

# Logs & temporary files
*.log
*.tmp
*.swp
*.bak

# Helm
charts/**/charts/
charts/**/tmpcharts/
charts/**/values.yaml~
charts/**/Chart.lock

# Kubernetes
*.out
*.orig
*.rej

# Kustomize build output
overlays/**/build/
overlays/**/kustomization.yaml.backup

# Terraform (optional if using)
*.tfstate
*.tfstate.backup
.terraform/
.terraform.lock.hcl

# Python (if scripting in CI)
__pycache__/
*.py[cod]

# Secrets (precaution)
secrets/**/*.yaml
!secrets/**/README.md   # allow README stubs

.git*

----------------------------------------------
32.  README.md
----------------------------------------------

[EMPTY FILE]


----------------------------------------------
33.  project-metadata.yaml
----------------------------------------------

[EMPTY FILE]


----------------------------------------------
34.  infra/aws/irsa/demo-api-service-account.yaml
----------------------------------------------




----------------------------------------------
35.  infra/aws/kms/sealed-secrets-kms-policy.json
----------------------------------------------

[EMPTY FILE]


----------------------------------------------
36.  tests/helm/demo-api-test.yaml
----------------------------------------------

[EMPTY FILE]


----------------------------------------------
37.  tests/kustomize/dev-overlay-check.sh
----------------------------------------------

[EMPTY FILE]


----------------------------------------------
38.  tests/secrets/validate-esosync.sh
----------------------------------------------

[EMPTY FILE]


----------------------------------------------
39.  delete-me-logs.txt
----------------------------------------------


==> Audit <==
|---------|---------------------------------------------------|----------|------|---------|---------------------|---------------------|
| Command |                       Args                        | Profile  | User | Version |     Start Time      |      End Time       |
|---------|---------------------------------------------------|----------|------|---------|---------------------|---------------------|
| stop    |                                                   | minikube | user | v1.35.0 | 23 May 25 00:16 EDT | 23 May 25 00:16 EDT |
| delete  |                                                   | minikube | user | v1.35.0 | 23 May 25 00:16 EDT | 23 May 25 00:16 EDT |
| start   | --driver=docker                                   | minikube | user | v1.35.0 | 23 May 25 00:16 EDT | 23 May 25 00:17 EDT |
| start   | --driver=docker                                   | minikube | user | v1.35.0 | 23 May 25 00:20 EDT | 23 May 25 00:20 EDT |
| image   | load my-vote-result:latest                        | minikube | user | v1.35.0 | 23 May 25 00:23 EDT | 23 May 25 00:24 EDT |
| image   | load my-vote-vote:latest                          | minikube | user | v1.35.0 | 23 May 25 00:24 EDT | 23 May 25 00:24 EDT |
| image   | load my-vote-worker:latest                        | minikube | user | v1.35.0 | 23 May 25 00:24 EDT | 23 May 25 00:24 EDT |
| ssh     |                                                   | minikube | user | v1.35.0 | 23 May 25 00:28 EDT |                     |
| ssh     |                                                   | minikube | user | v1.35.0 | 23 May 25 08:13 EDT | 23 May 25 08:13 EDT |
| ssh     |                                                   | minikube | user | v1.35.0 | 23 May 25 08:16 EDT | 23 May 25 08:17 EDT |
| ip      |                                                   | minikube | user | v1.35.0 | 23 May 25 08:28 EDT | 23 May 25 08:28 EDT |
| ip      |                                                   | minikube | user | v1.35.0 | 23 May 25 08:32 EDT | 23 May 25 08:32 EDT |
| ip      |                                                   | minikube | user | v1.35.0 | 23 May 25 08:32 EDT | 23 May 25 08:32 EDT |
| ip      |                                                   | minikube | user | v1.35.0 | 23 May 25 08:34 EDT | 23 May 25 08:34 EDT |
| ip      |                                                   | minikube | user | v1.35.0 | 23 May 25 08:37 EDT | 23 May 25 08:37 EDT |
| ip      |                                                   | minikube | user | v1.35.0 | 23 May 25 08:40 EDT | 23 May 25 08:40 EDT |
| ip      |                                                   | minikube | user | v1.35.0 | 23 May 25 08:49 EDT | 23 May 25 08:49 EDT |
| ip      |                                                   | minikube | user | v1.35.0 | 23 May 25 08:51 EDT | 23 May 25 08:51 EDT |
| ssh     |                                                   | minikube | user | v1.35.0 | 23 May 25 08:52 EDT | 23 May 25 08:54 EDT |
| ip      |                                                   | minikube | user | v1.35.0 | 23 May 25 08:59 EDT | 23 May 25 08:59 EDT |
| ssh     |                                                   | minikube | user | v1.35.0 | 23 May 25 09:04 EDT | 23 May 25 09:04 EDT |
| ssh     |                                                   | minikube | user | v1.35.0 | 23 May 25 09:07 EDT | 23 May 25 09:07 EDT |
| ssh     |                                                   | minikube | user | v1.35.0 | 23 May 25 09:07 EDT |                     |
| ip      |                                                   | minikube | user | v1.35.0 | 23 May 25 11:33 EDT | 23 May 25 11:33 EDT |
| service | kibana -n logging --url                           | minikube | user | v1.35.0 | 23 May 25 19:52 EDT | 23 May 25 19:52 EDT |
| ip      |                                                   | minikube | user | v1.35.0 | 23 May 25 19:54 EDT | 23 May 25 19:54 EDT |
| ip      |                                                   | minikube | user | v1.35.0 | 23 May 25 20:19 EDT | 23 May 25 20:19 EDT |
| ip      |                                                   | minikube | user | v1.35.0 | 23 May 25 20:25 EDT | 23 May 25 20:25 EDT |
| ip      |                                                   | minikube | user | v1.35.0 | 23 May 25 20:26 EDT | 23 May 25 20:26 EDT |
| ip      |                                                   | minikube | user | v1.35.0 | 23 May 25 20:33 EDT | 23 May 25 20:33 EDT |
| ip      |                                                   | minikube | user | v1.35.0 | 23 May 25 20:34 EDT | 23 May 25 20:34 EDT |
| ip      |                                                   | minikube | user | v1.35.0 | 23 May 25 20:46 EDT | 23 May 25 20:46 EDT |
| service | prometheus -n monitoring --url                    | minikube | user | v1.35.0 | 23 May 25 21:43 EDT |                     |
| service | prometheus -n monitoring --url                    | minikube | user | v1.35.0 | 23 May 25 21:43 EDT | 23 May 25 21:43 EDT |
| service | grafana -n monitoring --url                       | minikube | user | v1.35.0 | 23 May 25 21:43 EDT |                     |
| service | grafana -n monitoring --url                       | minikube | user | v1.35.0 | 23 May 25 21:43 EDT | 23 May 25 21:43 EDT |
| ip      |                                                   | minikube | user | v1.35.0 | 23 May 25 21:46 EDT | 23 May 25 21:46 EDT |
| start   |                                                   | minikube | user | v1.35.0 | 03 Jun 25 07:00 EDT | 03 Jun 25 07:00 EDT |
| ssh     | ls /var/lib/minikube/certs                        | minikube | user | v1.35.0 | 03 Jun 25 15:25 EDT | 03 Jun 25 15:25 EDT |
| ssh     | sudo ls                                           | minikube | user | v1.35.0 | 03 Jun 25 15:42 EDT | 03 Jun 25 15:42 EDT |
|         | /var/lib/minikube/certs                           |          |      |         |                     |                     |
| ssh     |                                                   | minikube | user | v1.35.0 | 03 Jun 25 16:01 EDT |                     |
| ssh     | -- cat                                            | minikube | user | v1.35.0 | 03 Jun 25 16:13 EDT | 03 Jun 25 16:13 EDT |
|         | /var/lib/minikube/certs/ca.crt                    |          |      |         |                     |                     |
| ssh     | -- cat                                            | minikube | user | v1.35.0 | 03 Jun 25 16:13 EDT |                     |
|         | /var/lib/minikube/certs/ca.key                    |          |      |         |                     |                     |
| ssh     | -- cat                                            | minikube | user | v1.35.0 | 03 Jun 25 16:17 EDT | 03 Jun 25 16:17 EDT |
|         | /var/lib/minikube/certs/ca.crt                    |          |      |         |                     |                     |
| ssh     | -- cat                                            | minikube | user | v1.35.0 | 03 Jun 25 16:17 EDT |                     |
|         | /var/lib/minikube/certs/ca.key                    |          |      |         |                     |                     |
| cp      | /var/lib/minikube/certs/ca.crt                    | minikube | user | v1.35.0 | 03 Jun 25 16:23 EDT |                     |
|         | ./ca.crt                                          |          |      |         |                     |                     |
| cp      | /var/lib/minikube/certs/ca.key                    | minikube | user | v1.35.0 | 03 Jun 25 16:23 EDT |                     |
|         | ./ca.key                                          |          |      |         |                     |                     |
| cp      | /var/lib/minikube/certs/ca.crt                    | minikube | user | v1.35.0 | 03 Jun 25 16:24 EDT |                     |
|         | /home/user/secure-secrets-platform-k8s-aws/ca.crt |          |      |         |                     |                     |
| cp      | /var/lib/minikube/certs/ca.key                    | minikube | user | v1.35.0 | 03 Jun 25 16:24 EDT |                     |
|         | /home/user/secure-secrets-platform-k8s-aws/ca.key |          |      |         |                     |                     |
| cp      | minikube:/var/lib/minikube/certs/ca.crt           | minikube | user | v1.35.0 | 03 Jun 25 16:26 EDT | 03 Jun 25 16:26 EDT |
|         | /home/user/secure-secrets-platform-k8s-aws/ca.crt |          |      |         |                     |                     |
| cp      | minikube:/var/lib/minikube/certs/ca.key           | minikube | user | v1.35.0 | 03 Jun 25 16:26 EDT |                     |
|         | /home/user/secure-secrets-platform-k8s-aws/ca.key |          |      |         |                     |                     |
| cp      | minikube:/var/lib/minikube/certs/ca.key           | minikube | user | v1.35.0 | 03 Jun 25 17:28 EDT |                     |
|         | /home/user/k8s-user-certs/../ca.key               |          |      |         |                     |                     |
| ssh     |                                                   | minikube | user | v1.35.0 | 03 Jun 25 17:29 EDT | 03 Jun 25 17:30 EDT |
| cp      | minikube:/home/docker/ca.key                      | minikube | user | v1.35.0 | 03 Jun 25 17:30 EDT | 03 Jun 25 17:30 EDT |
|         | /home/user/k8s-user-certs/../ca.key               |          |      |         |                     |                     |
| ssh     | -- rm /home/docker/ca.key                         | minikube | user | v1.35.0 | 03 Jun 25 17:30 EDT | 03 Jun 25 17:30 EDT |
| ssh     | -- rm /home/docker/ca.key                         | minikube | user | v1.35.0 | 03 Jun 25 17:31 EDT |                     |
| start   |                                                   | minikube | user | v1.35.0 | 03 Jun 25 18:44 EDT | 03 Jun 25 18:44 EDT |
| start   |                                                   | minikube | user | v1.35.0 | 06 Jun 25 17:08 EDT |                     |
| start   |                                                   | minikube | user | v1.35.0 | 06 Jun 25 17:09 EDT |                     |
| start   |                                                   | minikube | user | v1.35.0 | 06 Jun 25 17:28 EDT |                     |
|---------|---------------------------------------------------|----------|------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2025/06/06 17:28:48
Running on machine: minikubhost
Binary: Built with gc go1.23.4 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0606 17:28:48.005694    2650 out.go:345] Setting OutFile to fd 1 ...
I0606 17:28:48.005869    2650 out.go:397] isatty.IsTerminal(1) = true
I0606 17:28:48.005874    2650 out.go:358] Setting ErrFile to fd 2...
I0606 17:28:48.005879    2650 out.go:397] isatty.IsTerminal(2) = true
I0606 17:28:48.006063    2650 root.go:338] Updating PATH: /home/user/.minikube/bin
I0606 17:28:48.008046    2650 out.go:352] Setting JSON to false
I0606 17:28:48.009984    2650 start.go:129] hostinfo: {"hostname":"minikubhost","uptime":1069,"bootTime":1749244259,"procs":305,"os":"linux","platform":"rocky","platformFamily":"rhel","platformVersion":"9.5","kernelVersion":"5.14.0-503.14.1.el9_5.x86_64","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"4b6cbddb-09b0-4571-9848-284ebd473319"}
I0606 17:28:48.010732    2650 start.go:139] virtualization:  
I0606 17:28:48.011201    2650 out.go:177] üòÑ  minikube v1.35.0 on Rocky 9.5
I0606 17:28:48.012078    2650 out.go:177]     ‚ñ™ KUBECONFIG=/etc/kubernetes/admin.conf
I0606 17:28:48.012140    2650 notify.go:220] Checking for updates...
I0606 17:28:48.013572    2650 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0606 17:28:48.014392    2650 driver.go:394] Setting default libvirt URI to qemu:///system
I0606 17:28:48.094847    2650 docker.go:123] docker version: linux-28.1.1:Docker Engine - Community
I0606 17:28:48.094927    2650 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0606 17:28:48.270836    2650 info.go:266] docker info: {ID:75d70975-605d-4280-b5df-67e9468cd784 Containers:1 ContainersRunning:1 ContainersPaused:0 ContainersStopped:0 Images:10 Driver:overlay2 DriverStatus:[[Backing Filesystem xfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:24 OomKillDisable:false NGoroutines:44 SystemTime:2025-06-06 17:28:48.259914536 -0400 EDT LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:5.14.0-503.14.1.el9_5.x86_64 OperatingSystem:Rocky Linux 9.5 (Blue Onyx) OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:3801690112 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:minikubhost Labels:[] ExperimentalBuild:false ServerVersion:28.1.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.23.0] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.35.1]] Warnings:<nil>}}
I0606 17:28:48.270915    2650 docker.go:318] overlay module found
I0606 17:28:48.271558    2650 out.go:177] ‚ú®  Using the docker driver based on existing profile
I0606 17:28:48.271867    2650 start.go:297] selected driver: docker
I0606 17:28:48.271872    2650 start.go:901] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/user:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0606 17:28:48.271913    2650 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0606 17:28:48.272068    2650 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0606 17:28:48.316091    2650 info.go:266] docker info: {ID:75d70975-605d-4280-b5df-67e9468cd784 Containers:1 ContainersRunning:1 ContainersPaused:0 ContainersStopped:0 Images:10 Driver:overlay2 DriverStatus:[[Backing Filesystem xfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:24 OomKillDisable:false NGoroutines:44 SystemTime:2025-06-06 17:28:48.307062565 -0400 EDT LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:5.14.0-503.14.1.el9_5.x86_64 OperatingSystem:Rocky Linux 9.5 (Blue Onyx) OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:3801690112 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:minikubhost Labels:[] ExperimentalBuild:false ServerVersion:28.1.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.23.0] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.35.1]] Warnings:<nil>}}
I0606 17:28:48.316606    2650 cni.go:84] Creating CNI manager for ""
I0606 17:28:48.317174    2650 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0606 17:28:48.317220    2650 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/user:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0606 17:28:48.317665    2650 out.go:177] üëç  Starting "minikube" primary control-plane node in "minikube" cluster
I0606 17:28:48.318190    2650 cache.go:121] Beginning downloading kic base image for docker with docker
I0606 17:28:48.318594    2650 out.go:177] üöú  Pulling base image v0.0.46 ...
I0606 17:28:48.318897    2650 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0606 17:28:48.318941    2650 preload.go:146] Found local preload: /home/user/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4
I0606 17:28:48.318949    2650 cache.go:56] Caching tarball of preloaded images
I0606 17:28:48.318998    2650 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local docker daemon
I0606 17:28:48.319110    2650 preload.go:172] Found /home/user/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0606 17:28:48.319118    2650 cache.go:59] Finished verifying existence of preloaded tar for v1.32.0 on docker
I0606 17:28:48.319179    2650 profile.go:143] Saving config to /home/user/.minikube/profiles/minikube/config.json ...
I0606 17:28:48.344341    2650 image.go:100] Found gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local docker daemon, skipping pull
I0606 17:28:48.344351    2650 cache.go:145] gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 exists in daemon, skipping load
I0606 17:28:48.344359    2650 cache.go:227] Successfully downloaded all kic artifacts
I0606 17:28:48.344379    2650 start.go:360] acquireMachinesLock for minikube: {Name:mkbd622808407e995359d10389220c8264ecc388 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0606 17:28:48.344425    2650 start.go:364] duration metric: took 32.02¬µs to acquireMachinesLock for "minikube"
I0606 17:28:48.344439    2650 start.go:96] Skipping create...Using existing machine configuration
I0606 17:28:48.344450    2650 fix.go:54] fixHost starting: 
I0606 17:28:48.344702    2650 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0606 17:28:48.363099    2650 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0606 17:28:48.363143    2650 fix.go:138] unexpected machine state, will restart: <nil>
I0606 17:28:48.363614    2650 out.go:177] üîÑ  Restarting existing docker container for "minikube" ...
I0606 17:28:48.364047    2650 cli_runner.go:164] Run: docker start minikube
I0606 17:28:49.030940    2650 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0606 17:28:49.055386    2650 kic.go:430] container "minikube" state is running.
I0606 17:28:49.055649    2650 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0606 17:28:49.073620    2650 profile.go:143] Saving config to /home/user/.minikube/profiles/minikube/config.json ...
I0606 17:28:49.073804    2650 machine.go:93] provisionDockerMachine start ...
I0606 17:28:49.073873    2650 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0606 17:28:49.096767    2650 main.go:141] libmachine: Using SSH client type: native
I0606 17:28:49.097124    2650 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0606 17:28:49.097133    2650 main.go:141] libmachine: About to run SSH command:
hostname
I0606 17:28:49.097912    2650 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:33966->127.0.0.1:32768: read: connection reset by peer
I0606 17:28:52.266990    2650 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0606 17:28:52.267012    2650 ubuntu.go:169] provisioning hostname "minikube"
I0606 17:28:52.267138    2650 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0606 17:28:52.298344    2650 main.go:141] libmachine: Using SSH client type: native
I0606 17:28:52.298541    2650 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0606 17:28:52.298549    2650 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0606 17:28:52.495777    2650 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0606 17:28:52.495870    2650 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0606 17:28:52.516939    2650 main.go:141] libmachine: Using SSH client type: native
I0606 17:28:52.517058    2650 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0606 17:28:52.517067    2650 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0606 17:28:52.662223    2650 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0606 17:28:52.662330    2650 ubuntu.go:175] set auth options {CertDir:/home/user/.minikube CaCertPath:/home/user/.minikube/certs/ca.pem CaPrivateKeyPath:/home/user/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/user/.minikube/machines/server.pem ServerKeyPath:/home/user/.minikube/machines/server-key.pem ClientKeyPath:/home/user/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/user/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/user/.minikube}
I0606 17:28:52.662353    2650 ubuntu.go:177] setting up certificates
I0606 17:28:52.662367    2650 provision.go:84] configureAuth start
I0606 17:28:52.662475    2650 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0606 17:28:52.694011    2650 provision.go:143] copyHostCerts
I0606 17:28:52.694088    2650 exec_runner.go:144] found /home/user/.minikube/ca.pem, removing ...
I0606 17:28:52.694518    2650 exec_runner.go:203] rm: /home/user/.minikube/ca.pem
I0606 17:28:52.694558    2650 exec_runner.go:151] cp: /home/user/.minikube/certs/ca.pem --> /home/user/.minikube/ca.pem (1074 bytes)
I0606 17:28:52.694832    2650 exec_runner.go:144] found /home/user/.minikube/cert.pem, removing ...
I0606 17:28:52.694837    2650 exec_runner.go:203] rm: /home/user/.minikube/cert.pem
I0606 17:28:52.694853    2650 exec_runner.go:151] cp: /home/user/.minikube/certs/cert.pem --> /home/user/.minikube/cert.pem (1115 bytes)
I0606 17:28:52.695129    2650 exec_runner.go:144] found /home/user/.minikube/key.pem, removing ...
I0606 17:28:52.695133    2650 exec_runner.go:203] rm: /home/user/.minikube/key.pem
I0606 17:28:52.695163    2650 exec_runner.go:151] cp: /home/user/.minikube/certs/key.pem --> /home/user/.minikube/key.pem (1679 bytes)
I0606 17:28:52.695486    2650 provision.go:117] generating server cert: /home/user/.minikube/machines/server.pem ca-key=/home/user/.minikube/certs/ca.pem private-key=/home/user/.minikube/certs/ca-key.pem org=user.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0606 17:28:52.813778    2650 provision.go:177] copyRemoteCerts
I0606 17:28:52.814333    2650 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0606 17:28:52.814398    2650 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0606 17:28:52.833576    2650 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/user/.minikube/machines/minikube/id_rsa Username:docker}
I0606 17:28:52.938370    2650 ssh_runner.go:362] scp /home/user/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1074 bytes)
I0606 17:28:52.979569    2650 ssh_runner.go:362] scp /home/user/.minikube/machines/server.pem --> /etc/docker/server.pem (1176 bytes)
I0606 17:28:53.012550    2650 ssh_runner.go:362] scp /home/user/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0606 17:28:53.045041    2650 provision.go:87] duration metric: took 382.663832ms to configureAuth
I0606 17:28:53.045055    2650 ubuntu.go:193] setting minikube options for container-runtime
I0606 17:28:53.045184    2650 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0606 17:28:53.045236    2650 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0606 17:28:53.062982    2650 main.go:141] libmachine: Using SSH client type: native
I0606 17:28:53.063100    2650 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0606 17:28:53.063105    2650 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0606 17:28:53.203928    2650 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0606 17:28:53.203948    2650 ubuntu.go:71] root file system type: overlay
I0606 17:28:53.204106    2650 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0606 17:28:53.204259    2650 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0606 17:28:53.224169    2650 main.go:141] libmachine: Using SSH client type: native
I0606 17:28:53.224304    2650 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0606 17:28:53.224341    2650 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0606 17:28:53.392013    2650 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0606 17:28:53.392077    2650 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0606 17:28:53.411500    2650 main.go:141] libmachine: Using SSH client type: native
I0606 17:28:53.411624    2650 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0606 17:28:53.411634    2650 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0606 17:28:53.567695    2650 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0606 17:28:53.567715    2650 machine.go:96] duration metric: took 4.49390109s to provisionDockerMachine
I0606 17:28:53.567729    2650 start.go:293] postStartSetup for "minikube" (driver="docker")
I0606 17:28:53.567742    2650 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0606 17:28:53.567833    2650 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0606 17:28:53.567908    2650 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0606 17:28:53.586800    2650 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/user/.minikube/machines/minikube/id_rsa Username:docker}
I0606 17:28:53.689803    2650 ssh_runner.go:195] Run: cat /etc/os-release
I0606 17:28:53.694572    2650 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0606 17:28:53.694589    2650 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0606 17:28:53.694594    2650 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0606 17:28:53.694599    2650 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I0606 17:28:53.694607    2650 filesync.go:126] Scanning /home/user/.minikube/addons for local assets ...
I0606 17:28:53.694659    2650 filesync.go:126] Scanning /home/user/.minikube/files for local assets ...
I0606 17:28:53.694684    2650 start.go:296] duration metric: took 126.94978ms for postStartSetup
I0606 17:28:53.694744    2650 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0606 17:28:53.694784    2650 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0606 17:28:53.722263    2650 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/user/.minikube/machines/minikube/id_rsa Username:docker}
I0606 17:28:53.821548    2650 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0606 17:28:53.833397    2650 fix.go:56] duration metric: took 5.488939558s for fixHost
I0606 17:28:53.833415    2650 start.go:83] releasing machines lock for "minikube", held for 5.488983846s
I0606 17:28:53.833525    2650 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0606 17:28:53.864310    2650 ssh_runner.go:195] Run: cat /version.json
I0606 17:28:53.864382    2650 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0606 17:28:53.865214    2650 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0606 17:28:53.865281    2650 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0606 17:28:53.884162    2650 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/user/.minikube/machines/minikube/id_rsa Username:docker}
I0606 17:28:53.884423    2650 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/user/.minikube/machines/minikube/id_rsa Username:docker}
I0606 17:28:54.141878    2650 ssh_runner.go:195] Run: systemctl --version
I0606 17:28:54.154498    2650 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0606 17:28:54.163020    2650 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0606 17:28:54.187018    2650 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0606 17:28:54.187079    2650 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0606 17:28:54.197404    2650 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0606 17:28:54.197419    2650 start.go:495] detecting cgroup driver to use...
I0606 17:28:54.197511    2650 detect.go:190] detected "systemd" cgroup driver on host os
I0606 17:28:54.197708    2650 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0606 17:28:54.216731    2650 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0606 17:28:54.229440    2650 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0606 17:28:54.242036    2650 containerd.go:146] configuring containerd to use "systemd" as cgroup driver...
I0606 17:28:54.242091    2650 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = true|g' /etc/containerd/config.toml"
I0606 17:28:54.254416    2650 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0606 17:28:54.266132    2650 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0606 17:28:54.278741    2650 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0606 17:28:54.291402    2650 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0606 17:28:54.303476    2650 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0606 17:28:54.316196    2650 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0606 17:28:54.329837    2650 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0606 17:28:54.342298    2650 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0606 17:28:54.353460    2650 crio.go:166] couldn't verify netfilter by "sudo sysctl net.bridge.bridge-nf-call-iptables" which might be okay. error: sudo sysctl net.bridge.bridge-nf-call-iptables: Process exited with status 255
stdout:

stderr:
sysctl: cannot stat /proc/sys/net/bridge/bridge-nf-call-iptables: No such file or directory
I0606 17:28:54.353518    2650 ssh_runner.go:195] Run: sudo modprobe br_netfilter
I0606 17:28:54.371920    2650 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0606 17:28:54.383623    2650 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0606 17:28:54.462818    2650 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0606 17:28:54.544237    2650 start.go:495] detecting cgroup driver to use...
I0606 17:28:54.544283    2650 detect.go:190] detected "systemd" cgroup driver on host os
I0606 17:28:54.544334    2650 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0606 17:28:54.558743    2650 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0606 17:28:54.558798    2650 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0606 17:28:54.574390    2650 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0606 17:28:54.595665    2650 ssh_runner.go:195] Run: which cri-dockerd
I0606 17:28:54.600150    2650 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0606 17:28:54.612946    2650 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0606 17:28:54.635211    2650 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0606 17:28:54.718782    2650 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0606 17:28:54.800995    2650 docker.go:574] configuring docker to use "systemd" as cgroup driver...
I0606 17:28:54.801069    2650 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (129 bytes)
I0606 17:28:54.822815    2650 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0606 17:28:54.894271    2650 ssh_runner.go:195] Run: sudo systemctl restart docker
I0606 17:28:55.888694    2650 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0606 17:28:55.901737    2650 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I0606 17:28:55.914852    2650 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0606 17:28:55.929671    2650 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0606 17:28:56.010031    2650 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0606 17:28:56.081440    2650 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0606 17:28:56.157707    2650 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0606 17:28:56.181988    2650 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0606 17:28:56.195746    2650 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0606 17:28:56.269195    2650 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0606 17:28:56.475544    2650 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0606 17:28:56.475617    2650 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0606 17:28:56.480072    2650 start.go:563] Will wait 60s for crictl version
I0606 17:28:56.480115    2650 ssh_runner.go:195] Run: which crictl
I0606 17:28:56.484692    2650 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0606 17:28:56.581963    2650 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.4.1
RuntimeApiVersion:  v1
I0606 17:28:56.582075    2650 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0606 17:28:56.685653    2650 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0606 17:28:56.717428    2650 out.go:235] üê≥  Preparing Kubernetes v1.32.0 on Docker 27.4.1 ...
I0606 17:28:56.717565    2650 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0606 17:28:56.732615    2650 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I0606 17:28:56.740597    2650 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0606 17:28:56.756974    2650 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/user:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0606 17:28:56.757162    2650 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0606 17:28:56.757220    2650 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0606 17:28:56.800536    2650 docker.go:689] Got preloaded images: -- stdout --
my-vote-worker:latest
my-vote-result:latest
my-vote-vote:latest
redis:alpine
postgres:15-alpine
bitnami/sealed-secrets-controller:0.29.0
curlimages/curl:latest
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
busybox:latest
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
prom/prometheus:v2.52.0
grafana/grafana:10.3.1
fluent/fluent-bit:2.2.2
docker.elastic.co/elasticsearch/elasticsearch:8.11.1
docker.elastic.co/kibana/kibana:8.11.1
fluent/fluent-bit:1.8.12
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0606 17:28:56.800547    2650 docker.go:619] Images already preloaded, skipping extraction
I0606 17:28:56.800604    2650 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0606 17:28:56.830241    2650 docker.go:689] Got preloaded images: -- stdout --
my-vote-worker:latest
my-vote-result:latest
my-vote-vote:latest
redis:alpine
postgres:15-alpine
bitnami/sealed-secrets-controller:0.29.0
curlimages/curl:latest
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
busybox:latest
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
prom/prometheus:v2.52.0
grafana/grafana:10.3.1
fluent/fluent-bit:2.2.2
docker.elastic.co/elasticsearch/elasticsearch:8.11.1
docker.elastic.co/kibana/kibana:8.11.1
fluent/fluent-bit:1.8.12
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0606 17:28:56.830261    2650 cache_images.go:84] Images are preloaded, skipping loading
I0606 17:28:56.830268    2650 kubeadm.go:934] updating node { 192.168.49.2 8443 v1.32.0 docker true true} ...
I0606 17:28:56.830477    2650 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.32.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0606 17:28:56.830530    2650 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0606 17:28:56.993860    2650 cni.go:84] Creating CNI manager for ""
I0606 17:28:56.993872    2650 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0606 17:28:56.993942    2650 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0606 17:28:56.993958    2650 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.32.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0606 17:28:56.994045    2650 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      - name: "proxy-refresh-interval"
        value: "70000"
kubernetesVersion: v1.32.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0606 17:28:56.994104    2650 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.32.0
I0606 17:28:57.006416    2650 binaries.go:44] Found k8s binaries, skipping transfer
I0606 17:28:57.006473    2650 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0606 17:28:57.016759    2650 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0606 17:28:57.039197    2650 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0606 17:28:57.059844    2650 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2285 bytes)
I0606 17:28:57.081413    2650 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0606 17:28:57.086100    2650 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0606 17:28:57.099081    2650 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0606 17:28:57.181789    2650 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0606 17:28:57.206894    2650 certs.go:68] Setting up /home/user/.minikube/profiles/minikube for IP: 192.168.49.2
I0606 17:28:57.206904    2650 certs.go:194] generating shared ca certs ...
I0606 17:28:57.206914    2650 certs.go:226] acquiring lock for ca certs: {Name:mke32e43b48026f434383aa025803b9c56ebd98b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0606 17:28:57.207746    2650 certs.go:235] skipping valid "minikubeCA" ca cert: /home/user/.minikube/ca.key
I0606 17:28:57.208135    2650 certs.go:235] skipping valid "proxyClientCA" ca cert: /home/user/.minikube/proxy-client-ca.key
I0606 17:28:57.208145    2650 certs.go:256] generating profile certs ...
I0606 17:28:57.208657    2650 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": /home/user/.minikube/profiles/minikube/client.key
I0606 17:28:57.209005    2650 certs.go:359] skipping valid signed profile cert regeneration for "minikube": /home/user/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I0606 17:28:57.209444    2650 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": /home/user/.minikube/profiles/minikube/proxy-client.key
I0606 17:28:57.209588    2650 certs.go:484] found cert: /home/user/.minikube/certs/ca-key.pem (1675 bytes)
I0606 17:28:57.209620    2650 certs.go:484] found cert: /home/user/.minikube/certs/ca.pem (1074 bytes)
I0606 17:28:57.209646    2650 certs.go:484] found cert: /home/user/.minikube/certs/cert.pem (1115 bytes)
I0606 17:28:57.209669    2650 certs.go:484] found cert: /home/user/.minikube/certs/key.pem (1679 bytes)
I0606 17:28:57.210066    2650 ssh_runner.go:362] scp /home/user/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0606 17:28:57.270962    2650 ssh_runner.go:362] scp /home/user/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0606 17:28:57.307595    2650 ssh_runner.go:362] scp /home/user/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0606 17:28:57.339662    2650 ssh_runner.go:362] scp /home/user/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0606 17:28:57.368730    2650 ssh_runner.go:362] scp /home/user/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0606 17:28:57.399142    2650 ssh_runner.go:362] scp /home/user/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0606 17:28:57.429372    2650 ssh_runner.go:362] scp /home/user/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0606 17:28:57.460799    2650 ssh_runner.go:362] scp /home/user/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0606 17:28:57.498643    2650 ssh_runner.go:362] scp /home/user/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0606 17:28:57.528606    2650 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0606 17:28:57.554722    2650 ssh_runner.go:195] Run: openssl version
I0606 17:28:57.563515    2650 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0606 17:28:57.575861    2650 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0606 17:28:57.581754    2650 certs.go:528] hashing: -rw-r--r--. 1 root root 1111 May  1 22:05 /usr/share/ca-certificates/minikubeCA.pem
I0606 17:28:57.581804    2650 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0606 17:28:57.589075    2650 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0606 17:28:57.599688    2650 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0606 17:28:57.605108    2650 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0606 17:28:57.612699    2650 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0606 17:28:57.620918    2650 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0606 17:28:57.628005    2650 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0606 17:28:57.636149    2650 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0606 17:28:57.643591    2650 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0606 17:28:57.651030    2650 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/user:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0606 17:28:57.651130    2650 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0606 17:28:57.670114    2650 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0606 17:28:57.680571    2650 kubeadm.go:408] found existing configuration files, will attempt cluster restart
I0606 17:28:57.680578    2650 kubeadm.go:593] restartPrimaryControlPlane start ...
I0606 17:28:57.680623    2650 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0606 17:28:57.691200    2650 kubeadm.go:130] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0606 17:28:57.691268    2650 kubeconfig.go:47] verify endpoint returned: get endpoint: "minikube" does not appear in /etc/kubernetes/admin.conf
I0606 17:28:57.691302    2650 kubeconfig.go:62] /etc/kubernetes/admin.conf needs updating (will repair): [kubeconfig missing "minikube" cluster setting kubeconfig missing "minikube" context setting]
W0606 17:28:57.691512    2650 kubeadm.go:616] unable to update kubeconfig (cluster will likely require a reset): write kubeconfig: Error creating directory: /etc/kubernetes: mkdir /etc/kubernetes: permission denied
I0606 17:28:57.691631    2650 kubeadm.go:597] duration metric: took 11.049998ms to restartPrimaryControlPlane
W0606 17:28:57.691682    2650 out.go:270] ü§¶  Unable to restart control-plane node(s), will reset cluster: <no value>
I0606 17:28:57.691741    2650 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.32.0:$PATH" kubeadm reset --cri-socket /var/run/cri-dockerd.sock --force"
I0606 17:28:58.934206    2650 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.32.0:$PATH" kubeadm reset --cri-socket /var/run/cri-dockerd.sock --force": (1.242450341s)
I0606 17:28:58.934278    2650 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0606 17:28:58.946928    2650 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0606 17:28:58.957319    2650 kubeadm.go:214] ignoring SystemVerification for kubeadm because of docker driver
I0606 17:28:58.957363    2650 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0606 17:28:58.967741    2650 kubeadm.go:155] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0606 17:28:58.967750    2650 kubeadm.go:157] found existing configuration files:

I0606 17:28:58.967818    2650 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0606 17:28:58.978006    2650 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I0606 17:28:58.978061    2650 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I0606 17:28:58.991461    2650 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0606 17:28:59.003438    2650 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I0606 17:28:59.003483    2650 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I0606 17:28:59.013637    2650 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0606 17:28:59.024670    2650 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I0606 17:28:59.024729    2650 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0606 17:28:59.036753    2650 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0606 17:28:59.047188    2650 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I0606 17:28:59.047231    2650 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0606 17:28:59.058368    2650 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.32.0:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I0606 17:28:59.091871    2650 kubeadm.go:310] [init] Using Kubernetes version: v1.32.0
I0606 17:28:59.091906    2650 kubeadm.go:310] [preflight] Running pre-flight checks
I0606 17:28:59.161615    2650 kubeadm.go:310] [preflight] Pulling images required for setting up a Kubernetes cluster
I0606 17:28:59.161684    2650 kubeadm.go:310] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0606 17:28:59.161741    2650 kubeadm.go:310] [preflight] You can also perform this action beforehand using 'kubeadm config images pull'
I0606 17:28:59.171316    2650 kubeadm.go:310] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I0606 17:28:59.172088    2650 out.go:235]     ‚ñ™ Generating certificates and keys ...
I0606 17:28:59.172153    2650 kubeadm.go:310] [certs] Using existing ca certificate authority
I0606 17:28:59.172194    2650 kubeadm.go:310] [certs] Using existing apiserver certificate and key on disk
I0606 17:28:59.172245    2650 kubeadm.go:310] [certs] Using existing apiserver-kubelet-client certificate and key on disk
I0606 17:28:59.172330    2650 kubeadm.go:310] [certs] Using existing front-proxy-ca certificate authority
I0606 17:28:59.172378    2650 kubeadm.go:310] [certs] Using existing front-proxy-client certificate and key on disk
I0606 17:28:59.172687    2650 kubeadm.go:310] [certs] Using existing etcd/ca certificate authority
I0606 17:28:59.172728    2650 kubeadm.go:310] [certs] Using existing etcd/server certificate and key on disk
I0606 17:28:59.172766    2650 kubeadm.go:310] [certs] Using existing etcd/peer certificate and key on disk
I0606 17:28:59.172907    2650 kubeadm.go:310] [certs] Using existing etcd/healthcheck-client certificate and key on disk
I0606 17:28:59.173057    2650 kubeadm.go:310] [certs] Using existing apiserver-etcd-client certificate and key on disk
I0606 17:28:59.173085    2650 kubeadm.go:310] [certs] Using the existing "sa" key
I0606 17:28:59.173120    2650 kubeadm.go:310] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I0606 17:28:59.247479    2650 kubeadm.go:310] [kubeconfig] Writing "admin.conf" kubeconfig file
I0606 17:28:59.360709    2650 kubeadm.go:310] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I0606 17:28:59.457483    2650 kubeadm.go:310] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I0606 17:28:59.525678    2650 kubeadm.go:310] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I0606 17:28:59.637162    2650 kubeadm.go:310] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I0606 17:28:59.637481    2650 kubeadm.go:310] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I0606 17:28:59.640351    2650 kubeadm.go:310] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I0606 17:28:59.641162    2650 out.go:235]     ‚ñ™ Booting up control plane ...
I0606 17:28:59.641293    2650 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-apiserver"
I0606 17:28:59.641825    2650 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I0606 17:28:59.642360    2650 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-scheduler"
I0606 17:28:59.655728    2650 kubeadm.go:310] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I0606 17:28:59.664018    2650 kubeadm.go:310] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I0606 17:28:59.664226    2650 kubeadm.go:310] [kubelet-start] Starting the kubelet
I0606 17:28:59.759966    2650 kubeadm.go:310] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I0606 17:28:59.760107    2650 kubeadm.go:310] [kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
I0606 17:29:00.263835    2650 kubeadm.go:310] [kubelet-check] The kubelet is healthy after 503.589363ms
I0606 17:29:00.264041    2650 kubeadm.go:310] [api-check] Waiting for a healthy API server. This can take up to 4m0s
I0606 17:29:04.768237    2650 kubeadm.go:310] [api-check] The API server is healthy after 4.504483136s
I0606 17:29:04.783819    2650 kubeadm.go:310] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I0606 17:29:04.790890    2650 kubeadm.go:310] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I0606 17:29:04.805716    2650 kubeadm.go:310] [upload-certs] Skipping phase. Please see --upload-certs
I0606 17:29:04.805852    2650 kubeadm.go:310] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I0606 17:29:04.810578    2650 kubeadm.go:310] [bootstrap-token] Using token: cu7f1c.wk0yhghwumoxxqhj
I0606 17:29:04.811368    2650 out.go:235]     ‚ñ™ Configuring RBAC rules ...
I0606 17:29:04.811477    2650 kubeadm.go:310] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I0606 17:29:04.815913    2650 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I0606 17:29:04.820012    2650 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I0606 17:29:04.822414    2650 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I0606 17:29:04.825088    2650 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I0606 17:29:04.830352    2650 kubeadm.go:310] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I0606 17:29:05.177874    2650 kubeadm.go:310] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I0606 17:29:05.589736    2650 kubeadm.go:310] [addons] Applied essential addon: CoreDNS
I0606 17:29:06.183007    2650 kubeadm.go:310] [addons] Applied essential addon: kube-proxy
I0606 17:29:06.184770    2650 kubeadm.go:310] 
I0606 17:29:06.184874    2650 kubeadm.go:310] Your Kubernetes control-plane has initialized successfully!
I0606 17:29:06.184881    2650 kubeadm.go:310] 
I0606 17:29:06.185044    2650 kubeadm.go:310] To start using your cluster, you need to run the following as a regular user:
I0606 17:29:06.185051    2650 kubeadm.go:310] 
I0606 17:29:06.185088    2650 kubeadm.go:310]   mkdir -p $HOME/.kube
I0606 17:29:06.185175    2650 kubeadm.go:310]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I0606 17:29:06.185271    2650 kubeadm.go:310]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I0606 17:29:06.185280    2650 kubeadm.go:310] 
I0606 17:29:06.185427    2650 kubeadm.go:310] Alternatively, if you are the root user, you can run:
I0606 17:29:06.185433    2650 kubeadm.go:310] 
I0606 17:29:06.185504    2650 kubeadm.go:310]   export KUBECONFIG=/etc/kubernetes/admin.conf
I0606 17:29:06.185509    2650 kubeadm.go:310] 
I0606 17:29:06.185585    2650 kubeadm.go:310] You should now deploy a pod network to the cluster.
I0606 17:29:06.185680    2650 kubeadm.go:310] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I0606 17:29:06.185789    2650 kubeadm.go:310]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I0606 17:29:06.185794    2650 kubeadm.go:310] 
I0606 17:29:06.185915    2650 kubeadm.go:310] You can now join any number of control-plane nodes by copying certificate authorities
I0606 17:29:06.186003    2650 kubeadm.go:310] and service account keys on each node and then running the following as root:
I0606 17:29:06.186007    2650 kubeadm.go:310] 
I0606 17:29:06.186146    2650 kubeadm.go:310]   kubeadm join control-plane.minikube.internal:8443 --token cu7f1c.wk0yhghwumoxxqhj \
I0606 17:29:06.186319    2650 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:8f1e5ce385a48aa544a616d2a1483b1d92f414be9e2a267b11e9316d87e16085 \
I0606 17:29:06.186346    2650 kubeadm.go:310] 	--control-plane 
I0606 17:29:06.186350    2650 kubeadm.go:310] 
I0606 17:29:06.186463    2650 kubeadm.go:310] Then you can join any number of worker nodes by running the following on each as root:
I0606 17:29:06.186468    2650 kubeadm.go:310] 
I0606 17:29:06.186570    2650 kubeadm.go:310] kubeadm join control-plane.minikube.internal:8443 --token cu7f1c.wk0yhghwumoxxqhj \
I0606 17:29:06.186697    2650 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:8f1e5ce385a48aa544a616d2a1483b1d92f414be9e2a267b11e9316d87e16085 
I0606 17:29:06.188769    2650 kubeadm.go:310] 	[WARNING Swap]: swap is supported for cgroup v2 only. The kubelet must be properly configured to use swap. Please refer to https://kubernetes.io/docs/concepts/architecture/nodes/#swap-memory, or disable swap on the node
I0606 17:29:06.188878    2650 kubeadm.go:310] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I0606 17:29:06.188962    2650 kubeadm.go:310] 	[WARNING DirAvailable--var-lib-minikube-etcd]: /var/lib/minikube/etcd is not empty
I0606 17:29:06.188975    2650 cni.go:84] Creating CNI manager for ""
I0606 17:29:06.188989    2650 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0606 17:29:06.189802    2650 out.go:177] üîó  Configuring bridge CNI (Container Networking Interface) ...
I0606 17:29:06.190600    2650 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0606 17:29:06.204766    2650 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (496 bytes)
I0606 17:29:06.230643    2650 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0606 17:29:06.230753    2650 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.32.0/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I0606 17:29:06.231119    2650 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2025_06_06T17_29_06_0700 minikube.k8s.io/version=v1.35.0 minikube.k8s.io/commit=dd5d320e41b5451cdf3c01891bc4e13d189586ed-dirty minikube.k8s.io/name=minikube minikube.k8s.io/primary=true
I0606 17:29:06.242102    2650 ops.go:34] apiserver oom_adj: -16
I0606 17:29:06.410931    2650 kubeadm.go:1113] duration metric: took 180.222268ms to wait for elevateKubeSystemPrivileges
I0606 17:29:06.412611    2650 kubeadm.go:394] duration metric: took 8.761583973s to StartCluster
I0606 17:29:06.412630    2650 settings.go:142] acquiring lock: {Name:mkd828db7b7cf864e2d4d124744c6ee9c2490d8b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0606 17:29:06.413142    2650 settings.go:150] Updating kubeconfig:  /etc/kubernetes/admin.conf
I0606 17:29:06.414604    2650 out.go:201] 
W0606 17:29:06.414937    2650 out.go:270] ‚ùå  Exiting due to GUEST_START: failed to start node: Failed kubeconfig update: writing kubeconfig: Error creating directory: /etc/kubernetes: mkdir /etc/kubernetes: permission denied
W0606 17:29:06.414969    2650 out.go:270] 
W0606 17:29:06.415586    2650 out.go:293] [31m‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ[0m
[31m‚îÇ[0m                                                                                           [31m‚îÇ[0m
[31m‚îÇ[0m    üòø  If the above advice does not help, please let us know:                             [31m‚îÇ[0m
[31m‚îÇ[0m    üëâ  https://github.com/kubernetes/minikube/issues/new/choose                           [31m‚îÇ[0m
[31m‚îÇ[0m                                                                                           [31m‚îÇ[0m
[31m‚îÇ[0m    Please run `minikube logs --file=logs.txt` and attach logs.txt to the GitHub issue.    [31m‚îÇ[0m
[31m‚îÇ[0m                                                                                           [31m‚îÇ[0m
[31m‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ[0m
I0606 17:29:06.415904    2650 out.go:201] 


==> Docker <==
Jun 06 21:28:55 minikube dockerd[684]: time="2025-06-06T21:28:55.215596193Z" level=info msg="Daemon shutdown complete"
Jun 06 21:28:55 minikube systemd[1]: docker.service: Deactivated successfully.
Jun 06 21:28:55 minikube systemd[1]: Stopped Docker Application Container Engine.
Jun 06 21:28:55 minikube systemd[1]: Starting Docker Application Container Engine...
Jun 06 21:28:55 minikube dockerd[954]: time="2025-06-06T21:28:55.257018547Z" level=info msg="Starting up"
Jun 06 21:28:55 minikube dockerd[954]: time="2025-06-06T21:28:55.258719956Z" level=info msg="OTEL tracing is not configured, using no-op tracer provider"
Jun 06 21:28:55 minikube dockerd[954]: time="2025-06-06T21:28:55.273918154Z" level=info msg="[graphdriver] trying configured driver: overlay2"
Jun 06 21:28:55 minikube dockerd[954]: time="2025-06-06T21:28:55.297980183Z" level=info msg="Loading containers: start."
Jun 06 21:28:55 minikube dockerd[954]: time="2025-06-06T21:28:55.726836095Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Jun 06 21:28:55 minikube dockerd[954]: time="2025-06-06T21:28:55.850604867Z" level=warning msg="error locating sandbox id eadf8b9741e1007c9a817f147b29bc98fc4b1b21f26cd40530efa3149558e6f9: sandbox eadf8b9741e1007c9a817f147b29bc98fc4b1b21f26cd40530efa3149558e6f9 not found"
Jun 06 21:28:55 minikube dockerd[954]: time="2025-06-06T21:28:55.850650532Z" level=warning msg="error locating sandbox id e9480c5aeb6bdef10183827639c96076b0ee867d82a260ef08eb43fc9d1d846d: sandbox e9480c5aeb6bdef10183827639c96076b0ee867d82a260ef08eb43fc9d1d846d not found"
Jun 06 21:28:55 minikube dockerd[954]: time="2025-06-06T21:28:55.850663171Z" level=warning msg="error locating sandbox id 8f3f3e76a3873c84f563e68779fa7597d137d0d687a1077093c5a17bfce3e431: sandbox 8f3f3e76a3873c84f563e68779fa7597d137d0d687a1077093c5a17bfce3e431 not found"
Jun 06 21:28:55 minikube dockerd[954]: time="2025-06-06T21:28:55.850674540Z" level=warning msg="error locating sandbox id 82e1cdd515e279114c55b0d29233c9e979880a2b115f5a1a10c82e45ece7a3ff: sandbox 82e1cdd515e279114c55b0d29233c9e979880a2b115f5a1a10c82e45ece7a3ff not found"
Jun 06 21:28:55 minikube dockerd[954]: time="2025-06-06T21:28:55.850745804Z" level=warning msg="error locating sandbox id eeffaddbe19cc2ef867f4daa7bc737d87d49c1c4c7fb015dbe1340ac776b0ac0: sandbox eeffaddbe19cc2ef867f4daa7bc737d87d49c1c4c7fb015dbe1340ac776b0ac0 not found"
Jun 06 21:28:55 minikube dockerd[954]: time="2025-06-06T21:28:55.850757565Z" level=warning msg="error locating sandbox id 770e7c02c32c4ba29fb6be502ba1f40fe7bf845e2d24e12c0a1356504e9e4c2b: sandbox 770e7c02c32c4ba29fb6be502ba1f40fe7bf845e2d24e12c0a1356504e9e4c2b not found"
Jun 06 21:28:55 minikube dockerd[954]: time="2025-06-06T21:28:55.850767567Z" level=warning msg="error locating sandbox id 4fafdfe57b8a412a4af1d9269f06fd9d352e6b02045b2d47304c7fae016cbe3f: sandbox 4fafdfe57b8a412a4af1d9269f06fd9d352e6b02045b2d47304c7fae016cbe3f not found"
Jun 06 21:28:55 minikube dockerd[954]: time="2025-06-06T21:28:55.851006837Z" level=info msg="Loading containers: done."
Jun 06 21:28:55 minikube dockerd[954]: time="2025-06-06T21:28:55.859411740Z" level=info msg="Docker daemon" commit=c710b88 containerd-snapshotter=false storage-driver=overlay2 version=27.4.1
Jun 06 21:28:55 minikube dockerd[954]: time="2025-06-06T21:28:55.859516839Z" level=info msg="Daemon has completed initialization"
Jun 06 21:28:55 minikube dockerd[954]: time="2025-06-06T21:28:55.886519035Z" level=info msg="API listen on /var/run/docker.sock"
Jun 06 21:28:55 minikube dockerd[954]: time="2025-06-06T21:28:55.886585067Z" level=info msg="API listen on [::]:2376"
Jun 06 21:28:55 minikube systemd[1]: Started Docker Application Container Engine.
Jun 06 21:28:56 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Jun 06 21:28:56 minikube cri-dockerd[1231]: time="2025-06-06T21:28:56Z" level=info msg="Starting cri-dockerd dev (HEAD)"
Jun 06 21:28:56 minikube cri-dockerd[1231]: time="2025-06-06T21:28:56Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Jun 06 21:28:56 minikube cri-dockerd[1231]: time="2025-06-06T21:28:56Z" level=info msg="Start docker client with request timeout 0s"
Jun 06 21:28:56 minikube cri-dockerd[1231]: time="2025-06-06T21:28:56Z" level=info msg="Hairpin mode is set to hairpin-veth"
Jun 06 21:28:56 minikube cri-dockerd[1231]: time="2025-06-06T21:28:56Z" level=info msg="Loaded network plugin cni"
Jun 06 21:28:56 minikube cri-dockerd[1231]: time="2025-06-06T21:28:56Z" level=info msg="Docker cri networking managed by network plugin cni"
Jun 06 21:28:56 minikube cri-dockerd[1231]: time="2025-06-06T21:28:56Z" level=info msg="Setting cgroupDriver systemd"
Jun 06 21:28:56 minikube cri-dockerd[1231]: time="2025-06-06T21:28:56Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Jun 06 21:28:56 minikube cri-dockerd[1231]: time="2025-06-06T21:28:56Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Jun 06 21:28:56 minikube cri-dockerd[1231]: time="2025-06-06T21:28:56Z" level=info msg="Start cri-dockerd grpc backend"
Jun 06 21:28:56 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Jun 06 21:28:57 minikube cri-dockerd[1231]: time="2025-06-06T21:28:57Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-668d6bf9bc-9wrvt_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"e1300d48399f7d974d0507ea4eb7e4026e6b67938783ec4267567495746b7f86\""
Jun 06 21:28:57 minikube cri-dockerd[1231]: time="2025-06-06T21:28:57Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-668d6bf9bc-8dzmh_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"c30d264c6ad2e48037786ad8b7f64a40ff2ed8e1964a86ee0ef473b9fe2fd299\""
Jun 06 21:28:58 minikube cri-dockerd[1231]: time="2025-06-06T21:28:58Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/888cbc7ef5a702fc52e5c9d129517cfaafe0708d45a10a5056a16db49d1b8f30/resolv.conf as [nameserver 192.168.49.1 search localdomain options ndots:0]"
Jun 06 21:28:58 minikube cri-dockerd[1231]: time="2025-06-06T21:28:58Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d5288ce5aba673e4389e003af65ed192c87fffccbc005447a3713f021e358555/resolv.conf as [nameserver 192.168.49.1 search localdomain options ndots:0]"
Jun 06 21:28:58 minikube cri-dockerd[1231]: time="2025-06-06T21:28:58Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/99b14ab4f8d031d4b664856f2b981a00165121f58743f9f77a16ae20c1cec852/resolv.conf as [nameserver 192.168.49.1 search localdomain options ndots:0]"
Jun 06 21:28:58 minikube cri-dockerd[1231]: time="2025-06-06T21:28:58Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/93db0c27d83ca5e27e4b3a7310540d6a1ba08e4ae05b7a8561ef2f04d19b4baf/resolv.conf as [nameserver 192.168.49.1 search localdomain options ndots:0]"
Jun 06 21:28:58 minikube cri-dockerd[1231]: W0606 21:28:58.283332    1231 logging.go:59] [core] [Server #1] grpc: Server.processUnaryRPC failed to write status: connection error: desc = "transport is closing"
Jun 06 21:28:58 minikube cri-dockerd[1231]: W0606 21:28:58.286875    1231 logging.go:59] [core] [Server #1] grpc: Server.processUnaryRPC failed to write status: connection error: desc = "transport is closing"
Jun 06 21:28:58 minikube cri-dockerd[1231]: W0606 21:28:58.291468    1231 logging.go:59] [core] [Server #1] grpc: Server.processUnaryRPC failed to write status: connection error: desc = "transport is closing"
Jun 06 21:28:58 minikube dockerd[954]: time="2025-06-06T21:28:58.294466341Z" level=info msg="ignoring event" container=99b14ab4f8d031d4b664856f2b981a00165121f58743f9f77a16ae20c1cec852 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 06 21:28:58 minikube cri-dockerd[1231]: W0606 21:28:58.302532    1231 logging.go:59] [core] [Server #1] grpc: Server.processUnaryRPC failed to write status: connection error: desc = "transport is closing"
Jun 06 21:28:58 minikube dockerd[954]: time="2025-06-06T21:28:58.369548488Z" level=info msg="ignoring event" container=29c7729aabf06aef2f60aaac9f53b0619068e5bb4c25051b617f67fb1c3b2870 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 06 21:28:58 minikube dockerd[954]: time="2025-06-06T21:28:58.462968439Z" level=info msg="ignoring event" container=d5288ce5aba673e4389e003af65ed192c87fffccbc005447a3713f021e358555 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 06 21:28:58 minikube dockerd[954]: time="2025-06-06T21:28:58.528062380Z" level=info msg="ignoring event" container=cb338dda9529a8269c66726730271923cbd2f3a5496b64fa23726a781d9b0a30 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 06 21:28:58 minikube dockerd[954]: time="2025-06-06T21:28:58.598208161Z" level=info msg="ignoring event" container=93db0c27d83ca5e27e4b3a7310540d6a1ba08e4ae05b7a8561ef2f04d19b4baf module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 06 21:28:58 minikube dockerd[954]: time="2025-06-06T21:28:58.645355016Z" level=info msg="ignoring event" container=917938d3e465f95ea0d842d7308a44bf503aca50b3d9661c4876da49ba0d66f6 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 06 21:28:58 minikube dockerd[954]: time="2025-06-06T21:28:58.692490137Z" level=info msg="ignoring event" container=888cbc7ef5a702fc52e5c9d129517cfaafe0708d45a10a5056a16db49d1b8f30 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 06 21:28:58 minikube dockerd[954]: time="2025-06-06T21:28:58.756099888Z" level=info msg="ignoring event" container=b4fac96432e7d1fdd33483488bb6e752550fb476b2ca3cf1e07e5e8c134f5898 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 06 21:29:00 minikube cri-dockerd[1231]: time="2025-06-06T21:29:00Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/0fe4820eb9f93ff59388524e89946a2648159b236bdd1195a0f6768e00bd8ac1/resolv.conf as [nameserver 192.168.49.1 search localdomain options ndots:0]"
Jun 06 21:29:00 minikube cri-dockerd[1231]: time="2025-06-06T21:29:00Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/ccb54ad1f376ccd4d28aa42101d3150428bede3023cf260feab43e1699754231/resolv.conf as [nameserver 192.168.49.1 search localdomain options ndots:0]"
Jun 06 21:29:00 minikube cri-dockerd[1231]: time="2025-06-06T21:29:00Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/101e74219b9d3b147e2f9da184bbd26b9ac74b36eac96826f3fa5f4ca94c37f3/resolv.conf as [nameserver 192.168.49.1 search localdomain options ndots:0]"
Jun 06 21:29:00 minikube cri-dockerd[1231]: time="2025-06-06T21:29:00Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/19784a4d4928f9b43009342b1b7f62ced29cc745a921708720f07a0ad2ea2732/resolv.conf as [nameserver 192.168.49.1 search localdomain options ndots:0]"
Jun 06 21:29:10 minikube cri-dockerd[1231]: time="2025-06-06T21:29:10Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/8840cebe44ee274ff7158e47db007746e099befa6c2a4577a9533c5debe685db/resolv.conf as [nameserver 192.168.49.1 search localdomain options ndots:0]"
Jun 06 21:29:11 minikube cri-dockerd[1231]: time="2025-06-06T21:29:11Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d93c3715250f75fed48b8880cde59d3301778cf85f4ebc574690a0b1b211e46c/resolv.conf as [nameserver 192.168.49.1 search localdomain options ndots:0]"
Jun 06 21:29:11 minikube cri-dockerd[1231]: time="2025-06-06T21:29:11Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/0220d7e958e88f3c0e646d1528c0947466433253ef77fcb10c1b6e2619b88df0/resolv.conf as [nameserver 192.168.49.1 search localdomain options ndots:0]"
Jun 06 21:29:15 minikube cri-dockerd[1231]: time="2025-06-06T21:29:15Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"


==> container status <==
CONTAINER           IMAGE               CREATED              STATE               NAME                      ATTEMPT             POD ID              POD
db57f922d3dfd       c69fa2e9cbf5f       About a minute ago   Running             coredns                   0                   0220d7e958e88       coredns-668d6bf9bc-xxtqt
da1b1a1c44982       c69fa2e9cbf5f       About a minute ago   Running             coredns                   0                   d93c3715250f7       coredns-668d6bf9bc-m76bn
b157f7998f9d1       040f9f8aac8cd       About a minute ago   Running             kube-proxy                0                   8840cebe44ee2       kube-proxy-zwt7t
ff0e781062dda       a389e107f4ff1       About a minute ago   Running             kube-scheduler            0                   19784a4d4928f       kube-scheduler-minikube
4afff93ba2286       a9e7e6b294baf       About a minute ago   Running             etcd                      0                   101e74219b9d3       etcd-minikube
9f6d941f1c608       8cab3d2a8bd0f       About a minute ago   Running             kube-controller-manager   0                   ccb54ad1f376c       kube-controller-manager-minikube
088ffa05e7557       c2e17b8d0f4a3       About a minute ago   Running             kube-apiserver            0                   0fe4820eb9f93       kube-apiserver-minikube


==> coredns [da1b1a1c4498] <==
.:53
[INFO] plugin/reload: Running configuration SHA512 = 1b226df79860026c6a52e67daa10d7f0d57ec5b023288ec00c5e05f93523c894564e15b91770d3a07ae1cfbe861d15b37d4a0027e69c546ab112970993a3b03b
CoreDNS-1.11.3
linux/amd64, go1.21.11, a6338e9


==> coredns [db57f922d3df] <==
.:53
[INFO] plugin/reload: Running configuration SHA512 = 1b226df79860026c6a52e67daa10d7f0d57ec5b023288ec00c5e05f93523c894564e15b91770d3a07ae1cfbe861d15b37d4a0027e69c546ab112970993a3b03b
CoreDNS-1.11.3
linux/amd64, go1.21.11, a6338e9


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=dd5d320e41b5451cdf3c01891bc4e13d189586ed-dirty
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_06_06T17_29_06_0700
                    minikube.k8s.io/version=v1.35.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Fri, 06 Jun 2025 21:29:03 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Fri, 06 Jun 2025 21:30:16 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Fri, 06 Jun 2025 21:29:15 +0000   Fri, 06 Jun 2025 21:29:01 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Fri, 06 Jun 2025 21:29:15 +0000   Fri, 06 Jun 2025 21:29:01 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Fri, 06 Jun 2025 21:29:15 +0000   Fri, 06 Jun 2025 21:29:01 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Fri, 06 Jun 2025 21:29:15 +0000   Fri, 06 Jun 2025 21:29:03 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                8
  ephemeral-storage:  35256Mi
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3712588Ki
  pods:               110
Allocatable:
  cpu:                8
  ephemeral-storage:  35256Mi
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3712588Ki
  pods:               110
System Info:
  Machine ID:                 aa7828699ccf4daa81dc4f199f37df9a
  System UUID:                b76b0d1a-3848-403c-b09f-58af6041b63d
  Boot ID:                    2e15964b-f01d-458b-bd6b-e416650dabd1
  Kernel Version:             5.14.0-503.14.1.el9_5.x86_64
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://27.4.1
  Kubelet Version:            v1.32.0
  Kube-Proxy Version:         v1.32.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (7 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  kube-system                 coredns-668d6bf9bc-m76bn            100m (1%)     0 (0%)      70Mi (1%)        170Mi (4%)     67s
  kube-system                 coredns-668d6bf9bc-xxtqt            100m (1%)     0 (0%)      70Mi (1%)        170Mi (4%)     67s
  kube-system                 etcd-minikube                       100m (1%)     0 (0%)      100Mi (2%)       0 (0%)         72s
  kube-system                 kube-apiserver-minikube             250m (3%)     0 (0%)      0 (0%)           0 (0%)         72s
  kube-system                 kube-controller-manager-minikube    200m (2%)     0 (0%)      0 (0%)           0 (0%)         72s
  kube-system                 kube-proxy-zwt7t                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         67s
  kube-system                 kube-scheduler-minikube             100m (1%)     0 (0%)      0 (0%)           0 (0%)         72s
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                850m (10%)  0 (0%)
  memory             240Mi (6%)  340Mi (9%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type    Reason                   Age                From             Message
  ----    ------                   ----               ----             -------
  Normal  Starting                 66s                kube-proxy       
  Normal  Starting                 78s                kubelet          Starting kubelet.
  Normal  NodeHasSufficientMemory  77s (x8 over 77s)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    77s (x8 over 77s)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     77s (x7 over 77s)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  77s                kubelet          Updated Node Allocatable limit across pods
  Normal  Starting                 72s                kubelet          Starting kubelet.
  Normal  NodeAllocatableEnforced  72s                kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  72s                kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    72s                kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     72s                kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  RegisteredNode           68s                node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[Jun 6 21:10] core: CPUID marked event: 'cpu cycles' unavailable
[  +0.000001] core: CPUID marked event: 'instructions' unavailable
[  +0.000001] core: CPUID marked event: 'bus cycles' unavailable
[  +0.000001] core: CPUID marked event: 'cache references' unavailable
[  +0.000001] core: CPUID marked event: 'cache misses' unavailable
[  +0.000001] core: CPUID marked event: 'branch instructions' unavailable
[  +0.000001] core: CPUID marked event: 'branch misses' unavailable
[Jun 6 21:11] device-mapper: core: CONFIG_IMA_DISABLE_HTABLE is disabled. Duplicate IMA measurements will not be recorded in the IMA log.
[  +3.663973] piix4_smbus 0000:00:07.3: SMBus base address uninitialized - upgrade BIOS or use force_addr=0xaddr
[  +0.879616] Warning: Unmaintained driver is detected: ip_set
[  +1.434436] Warning: Unmaintained driver is detected: nft_compat
[  +0.068989] block dm-0: the capability attribute has been deprecated.
[  +0.102681] block nvme0n1: No UUID available providing old NGUID
[Jun 6 21:28] Warning: Unmaintained driver is detected: ip_tables


==> etcd [4afff93ba228] <==
{"level":"warn","ts":"2025-06-06T21:29:00.758159Z","caller":"embed/config.go:689","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2025-06-06T21:29:00.758247Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"info","ts":"2025-06-06T21:29:00.758492Z","caller":"etcdmain/etcd.go:116","msg":"server has been already initialized","data-dir":"/var/lib/minikube/etcd","dir-type":"member"}
{"level":"warn","ts":"2025-06-06T21:29:00.758514Z","caller":"embed/config.go:689","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2025-06-06T21:29:00.758521Z","caller":"embed/etcd.go:128","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2025-06-06T21:29:00.758540Z","caller":"embed/etcd.go:497","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-06-06T21:29:00.759485Z","caller":"embed/etcd.go:136","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2025-06-06T21:29:00.759658Z","caller":"embed/etcd.go:311","msg":"starting an etcd server","etcd-version":"3.5.16","git-sha":"f20bbad","go-version":"go1.22.7","go-os":"linux","go-arch":"amd64","max-cpu-set":8,"max-cpu-available":8,"member-initialized":true,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2025-06-06T21:29:00.790008Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"30.126903ms"}
{"level":"info","ts":"2025-06-06T21:29:00.790641Z","caller":"etcdserver/server.go:532","msg":"No snapshot found. Recovering WAL from scratch!"}
{"level":"info","ts":"2025-06-06T21:29:00.791514Z","caller":"etcdserver/raft.go:540","msg":"restarting local member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","commit-index":4}
{"level":"info","ts":"2025-06-06T21:29:00.792233Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=()"}
{"level":"info","ts":"2025-06-06T21:29:00.793632Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 2"}
{"level":"info","ts":"2025-06-06T21:29:00.793665Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [], term: 2, commit: 4, applied: 0, lastindex: 4, lastterm: 2]"}
{"level":"warn","ts":"2025-06-06T21:29:00.794207Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2025-06-06T21:29:00.794862Z","caller":"mvcc/kvstore.go:423","msg":"kvstore restored","current-rev":1}
{"level":"info","ts":"2025-06-06T21:29:00.795414Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2025-06-06T21:29:00.796420Z","caller":"etcdserver/corrupt.go:96","msg":"starting initial corruption check","local-member-id":"aec36adc501070cc","timeout":"7s"}
{"level":"info","ts":"2025-06-06T21:29:00.796452Z","caller":"etcdserver/corrupt.go:177","msg":"initial corruption checking passed; no corruption","local-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2025-06-06T21:29:00.796495Z","caller":"etcdserver/server.go:873","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.16","cluster-version":"to_be_decided"}
{"level":"info","ts":"2025-06-06T21:29:00.796627Z","caller":"etcdserver/server.go:773","msg":"starting initial election tick advance","election-ticks":10}
{"level":"info","ts":"2025-06-06T21:29:00.796692Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2025-06-06T21:29:00.796803Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2025-06-06T21:29:00.796819Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2025-06-06T21:29:00.796882Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-06-06T21:29:00.796923Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2025-06-06T21:29:00.797842Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","added-peer-id":"aec36adc501070cc","added-peer-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2025-06-06T21:29:00.797969Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2025-06-06T21:29:00.798041Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2025-06-06T21:29:00.801156Z","caller":"embed/etcd.go:729","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-06-06T21:29:00.801477Z","caller":"embed/etcd.go:280","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2025-06-06T21:29:00.801556Z","caller":"embed/etcd.go:871","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2025-06-06T21:29:00.801626Z","caller":"embed/etcd.go:600","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-06-06T21:29:00.801646Z","caller":"embed/etcd.go:572","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-06-06T21:29:02.494343Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 2"}
{"level":"info","ts":"2025-06-06T21:29:02.494518Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 2"}
{"level":"info","ts":"2025-06-06T21:29:02.494568Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 2"}
{"level":"info","ts":"2025-06-06T21:29:02.494614Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 3"}
{"level":"info","ts":"2025-06-06T21:29:02.494637Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 3"}
{"level":"info","ts":"2025-06-06T21:29:02.494668Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 3"}
{"level":"info","ts":"2025-06-06T21:29:02.494693Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 3"}
{"level":"info","ts":"2025-06-06T21:29:02.498852Z","caller":"etcdserver/server.go:2140","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2025-06-06T21:29:02.499234Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-06-06T21:29:02.499338Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-06-06T21:29:02.499625Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2025-06-06T21:29:02.499676Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2025-06-06T21:29:02.500477Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-06-06T21:29:02.500904Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-06-06T21:29:02.501736Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2025-06-06T21:29:02.502077Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}


==> kernel <==
 21:30:17 up 19 min,  0 users,  load average: 0.26, 0.11, 0.04
Linux minikube 5.14.0-503.14.1.el9_5.x86_64 #1 SMP PREEMPT_DYNAMIC Fri Nov 15 12:04:32 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [088ffa05e755] <==
I0606 21:29:03.225161       1 shared_informer.go:313] Waiting for caches to sync for configmaps
I0606 21:29:03.225268       1 remote_available_controller.go:411] Starting RemoteAvailability controller
I0606 21:29:03.225285       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I0606 21:29:03.225664       1 customresource_discovery_controller.go:292] Starting DiscoveryController
I0606 21:29:03.225706       1 controller.go:78] Starting OpenAPI AggregationController
I0606 21:29:03.237577       1 cluster_authentication_trust_controller.go:462] Starting cluster_authentication_trust_controller controller
I0606 21:29:03.237607       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I0606 21:29:03.237636       1 aggregator.go:169] waiting for initial CRD sync...
I0606 21:29:03.237662       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I0606 21:29:03.237668       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0606 21:29:03.238019       1 apf_controller.go:377] Starting API Priority and Fairness config controller
I0606 21:29:03.238522       1 local_available_controller.go:156] Starting LocalAvailability controller
I0606 21:29:03.238579       1 cache.go:32] Waiting for caches to sync for LocalAvailability controller
I0606 21:29:03.238604       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0606 21:29:03.238655       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0606 21:29:03.238899       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0606 21:29:03.238955       1 naming_controller.go:294] Starting NamingConditionController
I0606 21:29:03.239181       1 establishing_controller.go:81] Starting EstablishingController
I0606 21:29:03.239251       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I0606 21:29:03.239342       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0606 21:29:03.239401       1 crd_finalizer.go:269] Starting CRDFinalizer
I0606 21:29:03.238959       1 controller.go:90] Starting OpenAPI V3 controller
I0606 21:29:03.239972       1 controller.go:142] Starting OpenAPI controller
I0606 21:29:03.240505       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I0606 21:29:03.240517       1 system_namespaces_controller.go:66] Starting system namespaces controller
I0606 21:29:03.240524       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I0606 21:29:03.274569       1 shared_informer.go:320] Caches are synced for node_authorizer
I0606 21:29:03.286481       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0606 21:29:03.286555       1 policy_source.go:240] refreshing policies
E0606 21:29:03.322699       1 controller.go:145] "Failed to ensure lease exists, will retry" err="namespaces \"kube-system\" not found" interval="200ms"
I0606 21:29:03.325701       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0606 21:29:03.325829       1 shared_informer.go:320] Caches are synced for configmaps
I0606 21:29:03.337718       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0606 21:29:03.337814       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0606 21:29:03.337946       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I0606 21:29:03.338128       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0606 21:29:03.338160       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0606 21:29:03.338654       1 cache.go:39] Caches are synced for LocalAvailability controller
I0606 21:29:03.343918       1 controller.go:615] quota admission added evaluator for: namespaces
I0606 21:29:03.346819       1 shared_informer.go:320] Caches are synced for crd-autoregister
I0606 21:29:03.346867       1 aggregator.go:171] initial CRD sync complete...
I0606 21:29:03.346939       1 autoregister_controller.go:144] Starting autoregister controller
I0606 21:29:03.346948       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0606 21:29:03.346955       1 cache.go:39] Caches are synced for autoregister controller
I0606 21:29:03.536211       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I0606 21:29:04.234209       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I0606 21:29:04.238672       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I0606 21:29:04.238702       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0606 21:29:04.579480       1 controller.go:615] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0606 21:29:04.606635       1 controller.go:615] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0606 21:29:04.647045       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0606 21:29:04.652350       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0606 21:29:04.653204       1 controller.go:615] quota admission added evaluator for: endpoints
I0606 21:29:04.656034       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0606 21:29:05.271951       1 controller.go:615] quota admission added evaluator for: serviceaccounts
I0606 21:29:05.581873       1 controller.go:615] quota admission added evaluator for: deployments.apps
I0606 21:29:05.588963       1 alloc.go:330] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.96.0.10"}
I0606 21:29:05.594204       1 controller.go:615] quota admission added evaluator for: daemonsets.apps
I0606 21:29:10.421871       1 controller.go:615] quota admission added evaluator for: controllerrevisions.apps
I0606 21:29:10.470593       1 controller.go:615] quota admission added evaluator for: replicasets.apps


==> kube-controller-manager [9f6d941f1c60] <==
I0606 21:29:09.772552       1 shared_informer.go:320] Caches are synced for taint
I0606 21:29:09.772595       1 shared_informer.go:320] Caches are synced for daemon sets
I0606 21:29:09.772643       1 node_lifecycle_controller.go:1234] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0606 21:29:09.772702       1 shared_informer.go:320] Caches are synced for TTL after finished
I0606 21:29:09.772841       1 node_lifecycle_controller.go:886] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0606 21:29:09.772983       1 shared_informer.go:320] Caches are synced for persistent volume
I0606 21:29:09.772987       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0606 21:29:09.772935       1 node_lifecycle_controller.go:1080] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0606 21:29:09.773126       1 shared_informer.go:320] Caches are synced for bootstrap_signer
I0606 21:29:09.773173       1 shared_informer.go:320] Caches are synced for crt configmap
I0606 21:29:09.773189       1 shared_informer.go:320] Caches are synced for ReplicationController
I0606 21:29:09.774029       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-legacy-unknown
I0606 21:29:09.775269       1 shared_informer.go:320] Caches are synced for attach detach
I0606 21:29:09.776269       1 shared_informer.go:320] Caches are synced for endpoint_slice_mirroring
I0606 21:29:09.777759       1 shared_informer.go:320] Caches are synced for endpoint_slice
I0606 21:29:09.784416       1 shared_informer.go:320] Caches are synced for resource quota
I0606 21:29:09.786664       1 shared_informer.go:320] Caches are synced for HPA
I0606 21:29:09.792196       1 shared_informer.go:320] Caches are synced for certificate-csrapproving
I0606 21:29:09.801835       1 shared_informer.go:320] Caches are synced for ephemeral
I0606 21:29:09.808489       1 shared_informer.go:320] Caches are synced for taint-eviction-controller
I0606 21:29:09.814215       1 shared_informer.go:320] Caches are synced for endpoint
I0606 21:29:09.818556       1 shared_informer.go:320] Caches are synced for garbage collector
I0606 21:29:09.818576       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0606 21:29:09.818585       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I0606 21:29:09.818595       1 shared_informer.go:320] Caches are synced for cronjob
I0606 21:29:09.819367       1 shared_informer.go:320] Caches are synced for ClusterRoleAggregator
I0606 21:29:09.821126       1 shared_informer.go:320] Caches are synced for ReplicaSet
I0606 21:29:09.821138       1 shared_informer.go:320] Caches are synced for PVC protection
I0606 21:29:09.823403       1 shared_informer.go:320] Caches are synced for TTL
I0606 21:29:09.823433       1 shared_informer.go:320] Caches are synced for PV protection
I0606 21:29:09.823442       1 shared_informer.go:320] Caches are synced for disruption
I0606 21:29:09.823457       1 shared_informer.go:320] Caches are synced for legacy-service-account-token-cleaner
I0606 21:29:09.824119       1 shared_informer.go:320] Caches are synced for service account
I0606 21:29:09.825636       1 shared_informer.go:320] Caches are synced for expand
I0606 21:29:09.825693       1 shared_informer.go:320] Caches are synced for job
I0606 21:29:09.827124       1 shared_informer.go:320] Caches are synced for namespace
I0606 21:29:09.830317       1 shared_informer.go:320] Caches are synced for GC
I0606 21:29:09.831540       1 shared_informer.go:320] Caches are synced for resource quota
I0606 21:29:09.832661       1 shared_informer.go:320] Caches are synced for node
I0606 21:29:09.832754       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I0606 21:29:09.832802       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I0606 21:29:09.832827       1 shared_informer.go:313] Waiting for caches to sync for cidrallocator
I0606 21:29:09.832875       1 shared_informer.go:320] Caches are synced for cidrallocator
I0606 21:29:09.838514       1 range_allocator.go:428] "Set node PodCIDR" logger="node-ipam-controller" node="minikube" podCIDRs=["10.244.0.0/24"]
I0606 21:29:09.838603       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0606 21:29:09.838637       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0606 21:29:09.845937       1 shared_informer.go:320] Caches are synced for garbage collector
I0606 21:29:10.776183       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0606 21:29:10.933023       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="459.973781ms"
I0606 21:29:10.936669       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="3.579018ms"
I0606 21:29:10.936755       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="40.055¬µs"
I0606 21:29:10.936966       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="46.979¬µs"
I0606 21:29:10.943107       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="41.63¬µs"
I0606 21:29:12.573266       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="69.449¬µs"
I0606 21:29:12.591880       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="37.96¬µs"
I0606 21:29:13.432974       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="9.215807ms"
I0606 21:29:13.433168       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="122.667¬µs"
I0606 21:29:13.629481       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="10.81549ms"
I0606 21:29:13.629669       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="139.566¬µs"
I0606 21:29:15.712165       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"


==> kube-proxy [b157f7998f9d] <==
I0606 21:29:11.090155       1 server_linux.go:66] "Using iptables proxy"
I0606 21:29:11.205523       1 server.go:698] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0606 21:29:11.205615       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0606 21:29:11.235677       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0606 21:29:11.235735       1 server_linux.go:170] "Using iptables Proxier"
I0606 21:29:11.239156       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E0606 21:29:11.244005       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E0606 21:29:11.246833       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I0606 21:29:11.247734       1 server.go:497] "Version info" version="v1.32.0"
I0606 21:29:11.247761       1 server.go:499] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
E0606 21:29:11.251639       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
E0606 21:29:11.253688       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
I0606 21:29:11.256087       1 config.go:199] "Starting service config controller"
I0606 21:29:11.256218       1 config.go:105] "Starting endpoint slice config controller"
I0606 21:29:11.256950       1 config.go:329] "Starting node config controller"
I0606 21:29:11.257045       1 shared_informer.go:313] Waiting for caches to sync for node config
I0606 21:29:11.257103       1 shared_informer.go:313] Waiting for caches to sync for service config
I0606 21:29:11.257377       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0606 21:29:11.357639       1 shared_informer.go:320] Caches are synced for node config
I0606 21:29:11.357704       1 shared_informer.go:320] Caches are synced for service config
I0606 21:29:11.358841       1 shared_informer.go:320] Caches are synced for endpoint slice config


==> kube-scheduler [ff0e781062dd] <==
W0606 21:29:03.271733       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0606 21:29:03.271774       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0606 21:29:03.271784       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0606 21:29:03.286262       1 server.go:166] "Starting Kubernetes Scheduler" version="v1.32.0"
I0606 21:29:03.286300       1 server.go:168] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0606 21:29:03.289325       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0606 21:29:03.289757       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0606 21:29:03.290549       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0606 21:29:03.291672       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
W0606 21:29:03.297591       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0606 21:29:03.297733       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W0606 21:29:03.299370       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0606 21:29:03.299478       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0606 21:29:03.299407       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0606 21:29:03.299565       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0606 21:29:03.299645       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0606 21:29:03.300271       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0606 21:29:03.299692       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0606 21:29:03.300327       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
E0606 21:29:03.300242       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
E0606 21:29:03.300357       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0606 21:29:03.299679       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0606 21:29:03.300511       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0606 21:29:03.299772       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0606 21:29:03.300587       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0606 21:29:03.299866       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0606 21:29:03.300661       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0606 21:29:03.299866       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0606 21:29:03.300693       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0606 21:29:03.299878       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0606 21:29:03.300710       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0606 21:29:03.299939       1 reflector.go:569] runtime/asm_amd64.s:1700: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0606 21:29:03.300726       1 reflector.go:166] "Unhandled Error" err="runtime/asm_amd64.s:1700: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
W0606 21:29:03.299961       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0606 21:29:03.300743       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0606 21:29:03.300065       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "volumeattachments" in API group "storage.k8s.io" at the cluster scope
E0606 21:29:03.300758       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.VolumeAttachment: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0606 21:29:03.300130       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0606 21:29:03.300773       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0606 21:29:03.300145       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0606 21:29:03.300787       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0606 21:29:04.199732       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0606 21:29:04.199915       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0606 21:29:04.207722       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0606 21:29:04.207869       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0606 21:29:04.227730       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0606 21:29:04.228023       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0606 21:29:04.300967       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0606 21:29:04.301005       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0606 21:29:04.334667       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "volumeattachments" in API group "storage.k8s.io" at the cluster scope
E0606 21:29:04.334708       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.VolumeAttachment: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0606 21:29:04.340093       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0606 21:29:04.340132       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0606 21:29:04.351711       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0606 21:29:04.351752       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0606 21:29:04.368171       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0606 21:29:04.368210       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0606 21:29:04.388694       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0606 21:29:04.388732       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
I0606 21:29:04.896694       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
Jun 06 21:29:05 minikube kubelet[2735]: I0606 21:29:05.454325    2735 cpu_manager.go:221] "Starting CPU manager" policy="none"
Jun 06 21:29:05 minikube kubelet[2735]: I0606 21:29:05.454340    2735 cpu_manager.go:222] "Reconciling" reconcilePeriod="10s"
Jun 06 21:29:05 minikube kubelet[2735]: I0606 21:29:05.454356    2735 state_mem.go:36] "Initialized new in-memory state store"
Jun 06 21:29:05 minikube kubelet[2735]: I0606 21:29:05.454475    2735 state_mem.go:88] "Updated default CPUSet" cpuSet=""
Jun 06 21:29:05 minikube kubelet[2735]: I0606 21:29:05.454484    2735 state_mem.go:96] "Updated CPUSet assignments" assignments={}
Jun 06 21:29:05 minikube kubelet[2735]: I0606 21:29:05.454500    2735 policy_none.go:49] "None policy: Start"
Jun 06 21:29:05 minikube kubelet[2735]: I0606 21:29:05.454509    2735 memory_manager.go:186] "Starting memorymanager" policy="None"
Jun 06 21:29:05 minikube kubelet[2735]: I0606 21:29:05.454519    2735 state_mem.go:35] "Initializing new in-memory state store"
Jun 06 21:29:05 minikube kubelet[2735]: I0606 21:29:05.454607    2735 state_mem.go:75] "Updated machine memory state"
Jun 06 21:29:05 minikube kubelet[2735]: I0606 21:29:05.459157    2735 manager.go:519] "Failed to read data from checkpoint" checkpoint="kubelet_internal_checkpoint" err="checkpoint is not found"
Jun 06 21:29:05 minikube kubelet[2735]: I0606 21:29:05.459328    2735 eviction_manager.go:189] "Eviction manager: starting control loop"
Jun 06 21:29:05 minikube kubelet[2735]: I0606 21:29:05.459353    2735 container_log_manager.go:189] "Initializing container log rotate workers" workers=1 monitorPeriod="10s"
Jun 06 21:29:05 minikube kubelet[2735]: I0606 21:29:05.459494    2735 plugin_manager.go:118] "Starting Kubelet Plugin Manager"
Jun 06 21:29:05 minikube kubelet[2735]: E0606 21:29:05.466584    2735 eviction_manager.go:267] "eviction manager: failed to check if we have separate container filesystem. Ignoring." err="no imagefs label for configured runtime"
Jun 06 21:29:05 minikube kubelet[2735]: I0606 21:29:05.544018    2735 kubelet.go:3200] "Creating a mirror pod for static pod" pod="kube-system/kube-scheduler-minikube"
Jun 06 21:29:05 minikube kubelet[2735]: I0606 21:29:05.544308    2735 kubelet.go:3200] "Creating a mirror pod for static pod" pod="kube-system/kube-controller-manager-minikube"
Jun 06 21:29:05 minikube kubelet[2735]: I0606 21:29:05.544390    2735 kubelet.go:3200] "Creating a mirror pod for static pod" pod="kube-system/kube-apiserver-minikube"
Jun 06 21:29:05 minikube kubelet[2735]: I0606 21:29:05.544406    2735 kubelet.go:3200] "Creating a mirror pod for static pod" pod="kube-system/etcd-minikube"
Jun 06 21:29:05 minikube kubelet[2735]: I0606 21:29:05.562230    2735 kubelet_node_status.go:76] "Attempting to register node" node="minikube"
Jun 06 21:29:05 minikube kubelet[2735]: I0606 21:29:05.570654    2735 kubelet_node_status.go:125] "Node was previously registered" node="minikube"
Jun 06 21:29:05 minikube kubelet[2735]: I0606 21:29:05.570738    2735 kubelet_node_status.go:79] "Successfully registered node" node="minikube"
Jun 06 21:29:05 minikube kubelet[2735]: I0606 21:29:05.617163    2735 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/d72d0a4cf4be077c9919d46b7358a5e8-etc-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"d72d0a4cf4be077c9919d46b7358a5e8\") " pod="kube-system/kube-apiserver-minikube"
Jun 06 21:29:05 minikube kubelet[2735]: I0606 21:29:05.617315    2735 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/d72d0a4cf4be077c9919d46b7358a5e8-usr-local-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"d72d0a4cf4be077c9919d46b7358a5e8\") " pod="kube-system/kube-apiserver-minikube"
Jun 06 21:29:05 minikube kubelet[2735]: I0606 21:29:05.617359    2735 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etcd-data\" (UniqueName: \"kubernetes.io/host-path/2b4b75c2a289008e0b381891e9683040-etcd-data\") pod \"etcd-minikube\" (UID: \"2b4b75c2a289008e0b381891e9683040\") " pod="kube-system/etcd-minikube"
Jun 06 21:29:05 minikube kubelet[2735]: I0606 21:29:05.617383    2735 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/843c74f7b3bc7d7040a05c31708a6a30-ca-certs\") pod \"kube-controller-manager-minikube\" (UID: \"843c74f7b3bc7d7040a05c31708a6a30\") " pod="kube-system/kube-controller-manager-minikube"
Jun 06 21:29:05 minikube kubelet[2735]: I0606 21:29:05.617455    2735 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/843c74f7b3bc7d7040a05c31708a6a30-etc-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"843c74f7b3bc7d7040a05c31708a6a30\") " pod="kube-system/kube-controller-manager-minikube"
Jun 06 21:29:05 minikube kubelet[2735]: I0606 21:29:05.617478    2735 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/843c74f7b3bc7d7040a05c31708a6a30-kubeconfig\") pod \"kube-controller-manager-minikube\" (UID: \"843c74f7b3bc7d7040a05c31708a6a30\") " pod="kube-system/kube-controller-manager-minikube"
Jun 06 21:29:05 minikube kubelet[2735]: I0606 21:29:05.617500    2735 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/d14ce008bee3a1f3bd7cf547688f9dfe-kubeconfig\") pod \"kube-scheduler-minikube\" (UID: \"d14ce008bee3a1f3bd7cf547688f9dfe\") " pod="kube-system/kube-scheduler-minikube"
Jun 06 21:29:05 minikube kubelet[2735]: I0606 21:29:05.617530    2735 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etcd-certs\" (UniqueName: \"kubernetes.io/host-path/2b4b75c2a289008e0b381891e9683040-etcd-certs\") pod \"etcd-minikube\" (UID: \"2b4b75c2a289008e0b381891e9683040\") " pod="kube-system/etcd-minikube"
Jun 06 21:29:05 minikube kubelet[2735]: I0606 21:29:05.617611    2735 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/d72d0a4cf4be077c9919d46b7358a5e8-k8s-certs\") pod \"kube-apiserver-minikube\" (UID: \"d72d0a4cf4be077c9919d46b7358a5e8\") " pod="kube-system/kube-apiserver-minikube"
Jun 06 21:29:05 minikube kubelet[2735]: I0606 21:29:05.617651    2735 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"flexvolume-dir\" (UniqueName: \"kubernetes.io/host-path/843c74f7b3bc7d7040a05c31708a6a30-flexvolume-dir\") pod \"kube-controller-manager-minikube\" (UID: \"843c74f7b3bc7d7040a05c31708a6a30\") " pod="kube-system/kube-controller-manager-minikube"
Jun 06 21:29:05 minikube kubelet[2735]: I0606 21:29:05.617683    2735 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/843c74f7b3bc7d7040a05c31708a6a30-k8s-certs\") pod \"kube-controller-manager-minikube\" (UID: \"843c74f7b3bc7d7040a05c31708a6a30\") " pod="kube-system/kube-controller-manager-minikube"
Jun 06 21:29:05 minikube kubelet[2735]: I0606 21:29:05.617719    2735 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/843c74f7b3bc7d7040a05c31708a6a30-usr-local-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"843c74f7b3bc7d7040a05c31708a6a30\") " pod="kube-system/kube-controller-manager-minikube"
Jun 06 21:29:05 minikube kubelet[2735]: I0606 21:29:05.617744    2735 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/843c74f7b3bc7d7040a05c31708a6a30-usr-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"843c74f7b3bc7d7040a05c31708a6a30\") " pod="kube-system/kube-controller-manager-minikube"
Jun 06 21:29:05 minikube kubelet[2735]: I0606 21:29:05.617767    2735 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/d72d0a4cf4be077c9919d46b7358a5e8-ca-certs\") pod \"kube-apiserver-minikube\" (UID: \"d72d0a4cf4be077c9919d46b7358a5e8\") " pod="kube-system/kube-apiserver-minikube"
Jun 06 21:29:05 minikube kubelet[2735]: I0606 21:29:05.617917    2735 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/d72d0a4cf4be077c9919d46b7358a5e8-usr-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"d72d0a4cf4be077c9919d46b7358a5e8\") " pod="kube-system/kube-apiserver-minikube"
Jun 06 21:29:06 minikube kubelet[2735]: I0606 21:29:06.408571    2735 apiserver.go:52] "Watching apiserver"
Jun 06 21:29:06 minikube kubelet[2735]: I0606 21:29:06.416576    2735 desired_state_of_world_populator.go:157] "Finished populating initial desired state of world"
Jun 06 21:29:06 minikube kubelet[2735]: I0606 21:29:06.470645    2735 kubelet.go:3200] "Creating a mirror pod for static pod" pod="kube-system/kube-scheduler-minikube"
Jun 06 21:29:06 minikube kubelet[2735]: I0606 21:29:06.470706    2735 kubelet.go:3200] "Creating a mirror pod for static pod" pod="kube-system/kube-apiserver-minikube"
Jun 06 21:29:06 minikube kubelet[2735]: E0606 21:29:06.474911    2735 kubelet.go:3202] "Failed creating a mirror pod" err="pods \"kube-scheduler-minikube\" already exists" pod="kube-system/kube-scheduler-minikube"
Jun 06 21:29:06 minikube kubelet[2735]: E0606 21:29:06.475942    2735 kubelet.go:3202] "Failed creating a mirror pod" err="pods \"kube-apiserver-minikube\" already exists" pod="kube-system/kube-apiserver-minikube"
Jun 06 21:29:06 minikube kubelet[2735]: I0606 21:29:06.484495    2735 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-scheduler-minikube" podStartSLOduration=1.484481193 podStartE2EDuration="1.484481193s" podCreationTimestamp="2025-06-06 21:29:05 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-06-06 21:29:06.484368481 +0000 UTC m=+1.134766237" watchObservedRunningTime="2025-06-06 21:29:06.484481193 +0000 UTC m=+1.134878925"
Jun 06 21:29:06 minikube kubelet[2735]: I0606 21:29:06.497696    2735 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-apiserver-minikube" podStartSLOduration=1.497680863 podStartE2EDuration="1.497680863s" podCreationTimestamp="2025-06-06 21:29:05 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-06-06 21:29:06.491626143 +0000 UTC m=+1.142023880" watchObservedRunningTime="2025-06-06 21:29:06.497680863 +0000 UTC m=+1.148078600"
Jun 06 21:29:06 minikube kubelet[2735]: I0606 21:29:06.503144    2735 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-controller-manager-minikube" podStartSLOduration=1.503130254 podStartE2EDuration="1.503130254s" podCreationTimestamp="2025-06-06 21:29:05 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-06-06 21:29:06.497869004 +0000 UTC m=+1.148266742" watchObservedRunningTime="2025-06-06 21:29:06.503130254 +0000 UTC m=+1.153527994"
Jun 06 21:29:06 minikube kubelet[2735]: I0606 21:29:06.508752    2735 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/etcd-minikube" podStartSLOduration=1.508737409 podStartE2EDuration="1.508737409s" podCreationTimestamp="2025-06-06 21:29:05 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-06-06 21:29:06.50323034 +0000 UTC m=+1.153628077" watchObservedRunningTime="2025-06-06 21:29:06.508737409 +0000 UTC m=+1.159135150"
Jun 06 21:29:10 minikube kubelet[2735]: I0606 21:29:10.454295    2735 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/cbfebc38-8107-43d6-ad11-86ffe47857f1-kube-proxy\") pod \"kube-proxy-zwt7t\" (UID: \"cbfebc38-8107-43d6-ad11-86ffe47857f1\") " pod="kube-system/kube-proxy-zwt7t"
Jun 06 21:29:10 minikube kubelet[2735]: I0606 21:29:10.454334    2735 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-qbq7l\" (UniqueName: \"kubernetes.io/projected/cbfebc38-8107-43d6-ad11-86ffe47857f1-kube-api-access-qbq7l\") pod \"kube-proxy-zwt7t\" (UID: \"cbfebc38-8107-43d6-ad11-86ffe47857f1\") " pod="kube-system/kube-proxy-zwt7t"
Jun 06 21:29:10 minikube kubelet[2735]: I0606 21:29:10.454363    2735 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/cbfebc38-8107-43d6-ad11-86ffe47857f1-lib-modules\") pod \"kube-proxy-zwt7t\" (UID: \"cbfebc38-8107-43d6-ad11-86ffe47857f1\") " pod="kube-system/kube-proxy-zwt7t"
Jun 06 21:29:10 minikube kubelet[2735]: I0606 21:29:10.454377    2735 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/cbfebc38-8107-43d6-ad11-86ffe47857f1-xtables-lock\") pod \"kube-proxy-zwt7t\" (UID: \"cbfebc38-8107-43d6-ad11-86ffe47857f1\") " pod="kube-system/kube-proxy-zwt7t"
Jun 06 21:29:10 minikube kubelet[2735]: I0606 21:29:10.960031    2735 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/934ecd99-8ee2-4bd0-a960-a79cb98ec080-config-volume\") pod \"coredns-668d6bf9bc-m76bn\" (UID: \"934ecd99-8ee2-4bd0-a960-a79cb98ec080\") " pod="kube-system/coredns-668d6bf9bc-m76bn"
Jun 06 21:29:10 minikube kubelet[2735]: I0606 21:29:10.960167    2735 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-gzh8x\" (UniqueName: \"kubernetes.io/projected/b138bc82-4892-482f-b947-7c0c12009200-kube-api-access-gzh8x\") pod \"coredns-668d6bf9bc-xxtqt\" (UID: \"b138bc82-4892-482f-b947-7c0c12009200\") " pod="kube-system/coredns-668d6bf9bc-xxtqt"
Jun 06 21:29:10 minikube kubelet[2735]: I0606 21:29:10.960225    2735 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/b138bc82-4892-482f-b947-7c0c12009200-config-volume\") pod \"coredns-668d6bf9bc-xxtqt\" (UID: \"b138bc82-4892-482f-b947-7c0c12009200\") " pod="kube-system/coredns-668d6bf9bc-xxtqt"
Jun 06 21:29:10 minikube kubelet[2735]: I0606 21:29:10.960280    2735 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-6lk86\" (UniqueName: \"kubernetes.io/projected/934ecd99-8ee2-4bd0-a960-a79cb98ec080-kube-api-access-6lk86\") pod \"coredns-668d6bf9bc-m76bn\" (UID: \"934ecd99-8ee2-4bd0-a960-a79cb98ec080\") " pod="kube-system/coredns-668d6bf9bc-m76bn"
Jun 06 21:29:11 minikube kubelet[2735]: I0606 21:29:11.516326    2735 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-proxy-zwt7t" podStartSLOduration=1.5163061230000001 podStartE2EDuration="1.516306123s" podCreationTimestamp="2025-06-06 21:29:10 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-06-06 21:29:11.516300026 +0000 UTC m=+6.166697769" watchObservedRunningTime="2025-06-06 21:29:11.516306123 +0000 UTC m=+6.166703864"
Jun 06 21:29:12 minikube kubelet[2735]: I0606 21:29:12.574428    2735 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/coredns-668d6bf9bc-xxtqt" podStartSLOduration=2.574397162 podStartE2EDuration="2.574397162s" podCreationTimestamp="2025-06-06 21:29:10 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-06-06 21:29:12.573138145 +0000 UTC m=+7.223535905" watchObservedRunningTime="2025-06-06 21:29:12.574397162 +0000 UTC m=+7.224794948"
Jun 06 21:29:13 minikube kubelet[2735]: I0606 21:29:13.423481    2735 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/coredns-668d6bf9bc-m76bn" podStartSLOduration=3.423443919 podStartE2EDuration="3.423443919s" podCreationTimestamp="2025-06-06 21:29:10 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-06-06 21:29:12.591610963 +0000 UTC m=+7.242008704" watchObservedRunningTime="2025-06-06 21:29:13.423443919 +0000 UTC m=+8.073841691"
Jun 06 21:29:13 minikube kubelet[2735]: I0606 21:29:13.588728    2735 prober_manager.go:312] "Failed to trigger a manual run" probe="Readiness"
Jun 06 21:29:15 minikube kubelet[2735]: I0606 21:29:15.696908    2735 kuberuntime_manager.go:1702] "Updating runtime config through cri with podcidr" CIDR="10.244.0.0/24"
Jun 06 21:29:15 minikube kubelet[2735]: I0606 21:29:15.698788    2735 kubelet_network.go:61] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.244.0.0/24"



